{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab9b3d2-74cf-4936-82fe-d241255c3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import LightGBMModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import mae, mse\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c498e967-559f-4e68-9321-5bf8962e30ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe ais_train.csv after pruning\n",
      "                                time      latitude     longitude      vesselId\n",
      "count                        1399413  1.399413e+06  1.399413e+06  1.399413e+06\n",
      "mean   2024-03-06 08:34:26.835677184  3.703542e+01  1.112335e+01  2.695627e+02\n",
      "min              2024-01-01 00:00:25 -4.753287e+01 -1.675409e+02  0.000000e+00\n",
      "25%              2024-02-03 09:35:12  3.468584e+01 -5.053500e+00  1.140000e+02\n",
      "50%              2024-03-07 19:52:24  4.292247e+01  4.221300e+00  2.290000e+02\n",
      "75%              2024-04-07 08:39:04  5.137469e+01  1.811785e+01  4.180000e+02\n",
      "max              2024-05-07 23:59:08  7.055720e+01  1.788054e+02  6.870000e+02\n",
      "std                              NaN  2.266437e+01  6.733346e+01  1.887221e+02\n",
      "Describe schedules_to_may_2024.csv\n",
      "        portLatitude  portLongitude\n",
      "count  131848.000000  131848.000000\n",
      "mean       28.021038       4.338822\n",
      "std        27.401476      80.059006\n",
      "min       -37.832778    -149.571389\n",
      "25%        19.208333     -76.558889\n",
      "50%        35.164167       3.207222\n",
      "75%        50.902500      39.299167\n",
      "max        68.795000     174.771111\n",
      "Describe ports.csv\n",
      "           portLon      portLat\n",
      "count  1329.000000  1329.000000\n",
      "mean     20.630528    35.146181\n",
      "std      67.458101    25.098764\n",
      "min    -173.300000   -53.794444\n",
      "25%      -4.933000    25.920000\n",
      "50%      13.933000    39.050000\n",
      "75%      44.583000    53.882778\n",
      "max     178.426111    71.643056\n",
      "Describe ais_test.csv\n",
      "                 ID      vesselId                           time  \\\n",
      "count  51739.000000  51739.000000                          51739   \n",
      "mean   25869.000000    256.230696  2024-05-10 13:03:28.400065536   \n",
      "min        0.000000      2.000000            2024-05-08 00:03:16   \n",
      "25%    12934.500000    107.000000            2024-05-09 07:58:06   \n",
      "50%    25869.000000    199.000000            2024-05-10 13:37:58   \n",
      "75%    38803.500000    378.000000     2024-05-11 18:16:44.500000   \n",
      "max    51738.000000    679.000000            2024-05-12 23:59:58   \n",
      "std    14935.907126    188.185188                            NaN   \n",
      "\n",
      "       scaling_factor  \n",
      "count    51739.000000  \n",
      "mean         0.198038  \n",
      "min          0.100000  \n",
      "25%          0.150000  \n",
      "50%          0.200000  \n",
      "75%          0.250000  \n",
      "max          0.300000  \n",
      "std          0.069887  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ais_train.csv:\n",
    "Index(['time', 'cog', 'sog', 'rot', 'heading', 'navstat', 'etaRaw', 'latitude',\n",
    "       'longitude', 'vesselId', 'portId', 'elapsed_time'], dtype='object')\n",
    "\"\"\"\n",
    "ais_train = pd.read_csv(\"ais_train.csv\", sep=\"|\")\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time']).dt.tz_localize(None)  # Make 'time' timezone-naive\n",
    "\n",
    "# map vessel ids\n",
    "vessel_mapping = {vessel: idx for idx, vessel in enumerate(ais_train['vesselId'].unique())}\n",
    "ais_train['vesselId'] = ais_train['vesselId'].map(vessel_mapping)\n",
    "\n",
    "ais_train = ais_train[ais_train['cog']!=360] # cog=360 is not available\n",
    "ais_train = ais_train[(ais_train['cog'] <= 360) | (ais_train['cog'] > 409.5)] # this range should not be used\n",
    "\n",
    "ais_train = ais_train[ais_train['heading'] != 511] # unavailable\n",
    "ais_train = ais_train[ais_train['sog'] < 25]\n",
    "\n",
    "# Map 'navstat' values\n",
    "ais_train['navstat'] = ais_train['navstat'].replace(8, 0)  # Under way sailing -> Under way using engine\n",
    "ais_train = ais_train[~((ais_train['navstat'].isin([1, 5])) & (ais_train['sog'] > 0))]\n",
    "ais_train = ais_train[~((ais_train['navstat'] == 2) & (ais_train['sog'] > 5))]\n",
    "ais_train = ais_train.drop(['cog', 'sog', 'rot', 'heading', 'navstat', 'etaRaw'], axis=1)\n",
    "\n",
    "\n",
    "print(\"Describe ais_train.csv after pruning\")\n",
    "print(ais_train.describe())\n",
    "\n",
    "\"\"\"\n",
    "schedules_to_may_2024.csv\n",
    "Index(['vesselId', 'shippingLineId', 'shippingLineName', 'arrivalDate',\n",
    "       'sailingDate', 'portName', 'portId', 'portLatitude', 'portLongitude'],\n",
    "      dtype='object')\n",
    "\"\"\"\n",
    "schedules = pd.read_csv(\"schedules_to_may_2024.csv\", sep=\"|\")\n",
    "print(\"Describe schedules_to_may_2024.csv\")\n",
    "print(schedules.describe())\n",
    "schedules['vesselId'] = schedules['vesselId'].map(vessel_mapping)\n",
    "schedules['sailingDate'] = pd.to_datetime(schedules['sailingDate']).dt.tz_localize(None)\n",
    "schedules['arrivalDate'] = pd.to_datetime(schedules['arrivalDate']).dt.tz_localize(None)\n",
    "schedules = schedules.dropna(subset=['portLatitude']) # drop nan values\n",
    "schedules = schedules.drop_duplicates() # many duplicate values\n",
    "\n",
    "\"\"\"\n",
    "ports.csv\n",
    "Index(['portId', 'name', 'portLocation', 'longitude', 'latitude', 'UN_LOCODE',\n",
    "       'countryName', 'ISO'],\n",
    "      dtype='object')\n",
    "\"\"\"\n",
    "ports = pd.read_csv('ports.csv', sep='|')\n",
    "ports = ports.drop('portLocation', axis=1)\n",
    "ports = ports.drop('UN_LOCODE', axis=1)\n",
    "ports = ports.drop('countryName', axis=1)\n",
    "ports = ports.drop('ISO', axis=1)\n",
    "ports = ports.drop('name', axis=1)\n",
    "ports = ports.rename(columns={'longitude': 'portLon', 'latitude': 'portLat'})\n",
    "print(\"Describe ports.csv\")\n",
    "print(ports.describe())\n",
    "\n",
    "\"\"\"\n",
    "ais_test.csv\n",
    "Index(['ID', 'vesselId', 'time', 'scaling_factor'], dtype='object')\n",
    "\"\"\"\n",
    "ais_test = pd.read_csv(\"ais_test.csv\") # sep=\",\"\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time']).dt.tz_localize(None)  # Make 'time' timezone-naive\n",
    "ais_test['vesselId'] = ais_test['vesselId'].map(vessel_mapping)\n",
    "print(\"Describe ais_test.csv\")\n",
    "print(ais_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed499e7-379b-46f2-8168-c7135388b043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating training data from schedules: 100%|██████████████████████| 47043/47043 [00:09<00:00, 5023.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       time   latitude   longitude  vesselId\n",
      "0       2024-01-01 00:00:25 -34.743700  -57.851300       0.0\n",
      "1       2024-01-01 00:00:36   8.894400  -79.479390       1.0\n",
      "2       2024-01-01 00:01:45  39.190650  -76.475670       2.0\n",
      "3       2024-01-01 00:03:11 -34.411890  151.020670       3.0\n",
      "4       2024-01-01 00:03:51  35.883790   -5.916360       4.0\n",
      "...                     ...        ...         ...       ...\n",
      "1446451 2024-02-19 00:00:00 -34.098889  -59.007778     342.0\n",
      "1446452 2023-12-24 00:00:00  53.563611    8.554722     309.0\n",
      "1446453 2023-12-22 00:00:00  50.902500   -1.428889     309.0\n",
      "1446454 2023-12-26 00:00:00  51.336389    3.207222     309.0\n",
      "1446455 2023-12-15 00:00:00  41.340278    2.164722     309.0\n",
      "\n",
      "[1441863 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an empty list to hold new rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate through each row in schedules\n",
    "for _, sail in tqdm(schedules.iterrows(), desc=\"Generating training data from schedules\", total=len(schedules)):\n",
    "    # Skip if portId is NaN\n",
    "    if pd.isna(sail['portId']):\n",
    "        continue\n",
    "    \n",
    "    # Find corresponding port information in the ports DataFrame\n",
    "    port_info = ports[ports['portId'] == sail['portId']]\n",
    "    if port_info.empty:\n",
    "        continue\n",
    "\n",
    "    # Extract port latitude and longitude\n",
    "    port_lat = port_info['portLat'].values[0]\n",
    "    port_lon = port_info['portLon'].values[0]\n",
    "\n",
    "    # Create a new row with the required data\n",
    "    new_row = {\n",
    "        'time': sail['arrivalDate'],  # or 'sailingDate' depending on what you need\n",
    "        'latitude': port_lat,\n",
    "        'longitude': port_lon,\n",
    "        'vesselId': sail['vesselId'],\n",
    "        'portId': sail['portId']\n",
    "    }\n",
    "\n",
    "    # Add the new row to the list of new rows\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# Convert the list of new rows to a DataFrame\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Concatenate the new rows DataFrame with ais_train\n",
    "ais_train = pd.concat([ais_train, new_rows_df], ignore_index=True)\n",
    "ais_train.dropna(subset=['vesselId'], inplace=True)\n",
    "ais_train = ais_train.drop(['portId'], axis=1)\n",
    "print(ais_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32519af5-0440-4d4b-8ee8-b19adae24b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of entries: 1441863\n",
      "Columns: ['time', 'latitude', 'longitude', 'vesselId']\n",
      "Number of NaT in 'time' column: 2726\n",
      "Sample rows with NaT in 'time':\n",
      "        time   latitude   longitude  vesselId\n",
      "1399486  NaT   1.292778  103.725278     532.0\n",
      "1399487  NaT  51.297778    4.299722     532.0\n",
      "1399488  NaT  35.562222  140.064444     532.0\n",
      "1399489  NaT  24.258333  120.506111     532.0\n",
      "1399490  NaT  51.498889   -2.712222     532.0\n",
      "After dropping NaT, number of entries: 1439137\n",
      "After dropping duplicates: 1427120\n",
      "DataFrame sorted by 'vesselId' and 'time'.\n",
      "Starting interpolation for each vessel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 0/688 [00:00<?, ?it/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_18798/4239506462.py:65: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  time_num = df['time'].view('int64') // 10**9  # Convert to seconds\n",
      "  0%|                                                                               | 0/688 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid frequency: 1day, failed to parse with error message: ValueError(\"Invalid frequency: DAY, failed to parse with error message: KeyError('DAY')\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32moffsets.pyx:4776\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.offsets._get_offset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DAY'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32moffsets.pyx:4946\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.offsets.to_offset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32moffsets.pyx:4782\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.offsets._get_offset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid frequency: DAY, failed to parse with error message: KeyError('DAY')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting interpolation for each vessel...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vessel_id, group \u001b[38;5;129;01min\u001b[39;00m tqdm(ais_train\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvesselId\u001b[39m\u001b[38;5;124m'\u001b[39m), total\u001b[38;5;241m=\u001b[39mais_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvesselId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()):\n\u001b[0;32m--> 135\u001b[0m     interp_df \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate_vessel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_time_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_time_resolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     interpolated_dfs\u001b[38;5;241m.\u001b[39mappend(interp_df)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Step 9: Concatenate All Interpolated DataFrames\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 84\u001b[0m, in \u001b[0;36minterpolate_vessel\u001b[0;34m(df, new_time_resolution)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Define the new time points based on the desired resolution\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m new_time \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_time_resolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Convert new_time to numerical format (seconds)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py:1008\u001b[0m, in \u001b[0;36mdate_range\u001b[0;34m(start, end, periods, freq, tz, normalize, name, inclusive, unit, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m com\u001b[38;5;241m.\u001b[39many_none(periods, start, end):\n\u001b[1;32m   1006\u001b[0m     freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1008\u001b[0m dtarr \u001b[38;5;241m=\u001b[39m \u001b[43mDatetimeArray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperiods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclusive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeIndex\u001b[38;5;241m.\u001b[39m_simple_new(dtarr, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniforge3/envs/py310/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:421\u001b[0m, in \u001b[0;36mDatetimeArray._generate_range\u001b[0;34m(cls, start, end, periods, freq, tz, normalize, ambiguous, nonexistent, inclusive, unit)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mcount_not_none(start, end, periods, freq) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOf the four parameters: start, end, periods, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand freq, exactly three must be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     )\n\u001b[0;32m--> 421\u001b[0m freq \u001b[38;5;241m=\u001b[39m \u001b[43mto_offset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     start \u001b[38;5;241m=\u001b[39m Timestamp(start)\n",
      "File \u001b[0;32moffsets.pyx:4791\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.offsets.to_offset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32moffsets.pyx:4954\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.offsets.to_offset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid frequency: 1day, failed to parse with error message: ValueError(\"Invalid frequency: DAY, failed to parse with error message: KeyError('DAY')\")"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming ais_train is already loaded as a DataFrame\n",
    "print(f\"Initial number of entries: {len(ais_train)}\")\n",
    "print(f\"Columns: {ais_train.columns.tolist()}\")\n",
    "\n",
    "# Step 1: Handle NaT Values in 'time' Column\n",
    "num_nat = ais_train['time'].isna().sum()\n",
    "print(f\"Number of NaT in 'time' column: {num_nat}\")\n",
    "\n",
    "if num_nat > 0:\n",
    "    print(\"Sample rows with NaT in 'time':\")\n",
    "    print(ais_train[ais_train['time'].isna()].head())\n",
    "    \n",
    "    # Drop rows where 'time' is NaT\n",
    "    ais_train = ais_train.dropna(subset=['time'])\n",
    "    print(f\"After dropping NaT, number of entries: {len(ais_train)}\")\n",
    "\n",
    "# Step 2: Remove Duplicate (vesselId, time) Pairs\n",
    "ais_train = ais_train.drop_duplicates(subset=['vesselId', 'time'])\n",
    "print(f\"After dropping duplicates: {len(ais_train)}\")\n",
    "\n",
    "# Step 3: Convert 'time' to datetime if it's not already\n",
    "if not np.issubdtype(ais_train['time'].dtype, np.datetime64):\n",
    "    ais_train['time'] = pd.to_datetime(ais_train['time'], errors='coerce')  # Coerce invalid formats to NaT\n",
    "    \n",
    "    # Check again for NaT after conversion\n",
    "    num_nat = ais_train['time'].isna().sum()\n",
    "    if num_nat > 0:\n",
    "        print(f\"Number of NaT after conversion: {num_nat}\")\n",
    "        print(\"Dropping rows with NaT after conversion.\")\n",
    "        ais_train = ais_train.dropna(subset=['time'])\n",
    "        print(f\"After dropping NaT post-conversion, number of entries: {len(ais_train)}\")\n",
    "\n",
    "# Step 4: Sort the DataFrame by 'vesselId' and 'time'\n",
    "ais_train = ais_train.sort_values(by=['vesselId', 'time'])\n",
    "print(\"DataFrame sorted by 'vesselId' and 'time'.\")\n",
    "\n",
    "# Step 5: Define the Interpolation Function with Enhanced Robustness\n",
    "def interpolate_vessel(df, new_time_resolution='1min'):\n",
    "    \"\"\"\n",
    "    Interpolate latitude and longitude for a single vessel.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing 'time', 'latitude', 'longitude' for a vessel\n",
    "    - new_time_resolution: String representing the new time frequency (e.g., '1T' for 1 minute)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with interpolated 'time', 'latitude', 'longitude', 'vesselId'\n",
    "    \"\"\"\n",
    "    # Ensure the data is sorted by time\n",
    "    df = df.sort_values('time')\n",
    "    \n",
    "    # Check for any NaT in 'time'\n",
    "    if df['time'].isna().any():\n",
    "        print(f\"Skipping vesselId {df['vesselId'].iloc[0]} due to NaT in 'time'\")\n",
    "        return df\n",
    "    \n",
    "    # Convert time to numerical format (e.g., timestamp in seconds)\n",
    "    try:\n",
    "        # Use .view('int64') instead of .astype(np.int64) for better compatibility\n",
    "        time_num = df['time'].view('int64') // 10**9  # Convert to seconds\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError for vesselId {df['vesselId'].iloc[0]}: {e}\")\n",
    "        return df\n",
    "    \n",
    "    # Define the new time range\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    \n",
    "    # Check if start_time and end_time are valid and not equal\n",
    "    if pd.isna(start_time) or pd.isna(end_time):\n",
    "        print(f\"Invalid start or end time for vesselId {df['vesselId'].iloc[0]}\")\n",
    "        return df\n",
    "    \n",
    "    if start_time == end_time:\n",
    "        print(f\"Start and end times are the same for vesselId {df['vesselId'].iloc[0]}\")\n",
    "        return df\n",
    "    \n",
    "    # Define the new time points based on the desired resolution\n",
    "    new_time = pd.date_range(start=start_time,\n",
    "                             end=end_time,\n",
    "                             freq=new_time_resolution)\n",
    "    \n",
    "    # Convert new_time to numerical format (seconds)\n",
    "    try:\n",
    "        new_time_num = new_time.view('int64') // 10**9  # Convert to seconds\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError when converting new_time for vesselId {df['vesselId'].iloc[0]}: {e}\")\n",
    "        return df\n",
    "    \n",
    "    # Handle cases with insufficient data points\n",
    "    if len(df) < 4:\n",
    "        # Cubic spline requires at least 4 points\n",
    "        print(f\"Insufficient data points for vesselId {df['vesselId'].iloc[0]}. Required: 4, Available: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    try:\n",
    "        # Create cubic spline interpolators\n",
    "        cs_lat = CubicSpline(time_num, df['latitude'])\n",
    "        cs_lon = CubicSpline(time_num, df['longitude'])\n",
    "        \n",
    "        # Interpolate latitude and longitude\n",
    "        interp_lat = cs_lat(new_time_num)\n",
    "        interp_lon = cs_lon(new_time_num)\n",
    "        \n",
    "        # Create the interpolated DataFrame\n",
    "        interp_df = pd.DataFrame({\n",
    "            'time': new_time,\n",
    "            'latitude': interp_lat,\n",
    "            'longitude': interp_lon,\n",
    "            'vesselId': df['vesselId'].iloc[0]\n",
    "        })\n",
    "        \n",
    "        return interp_df\n",
    "    except Exception as e:\n",
    "        # In case of any error (e.g., duplicate time points), return the original data\n",
    "        print(f\"Interpolation failed for vesselId {df['vesselId'].iloc[0]}: {e}\")\n",
    "        return df\n",
    "\n",
    "# Step 6: Define the Desired Time Resolution for Interpolation\n",
    "new_time_resolution = '1d'  # 1 minute intervals\n",
    "# Note: In your original code, you set '1d' with a comment indicating 1 minute intervals. \n",
    "#       Ensure the frequency string matches the intended resolution.\n",
    "\n",
    "# Step 7: Initialize a List to Hold Interpolated DataFrames\n",
    "interpolated_dfs = []\n",
    "\n",
    "# Step 8: Perform Interpolation Using a For Loop\n",
    "print(\"Starting interpolation for each vessel...\")\n",
    "for vessel_id, group in tqdm(ais_train.groupby('vesselId'), total=ais_train['vesselId'].nunique()):\n",
    "    interp_df = interpolate_vessel(group, new_time_resolution=new_time_resolution)\n",
    "    interpolated_dfs.append(interp_df)\n",
    "\n",
    "# Step 9: Concatenate All Interpolated DataFrames\n",
    "ais_interpolated = pd.concat(interpolated_dfs, ignore_index=True)\n",
    "print(\"Interpolation completed.\")\n",
    "\n",
    "print(f\"Original number of entries after preprocessing: {len(ais_train)}\")\n",
    "print(f\"Interpolated number of entries: {len(ais_interpolated)}\")\n",
    "print(\"Sample of interpolated data:\")\n",
    "print(ais_interpolated.head())\n",
    "\n",
    "print(f\"Final number of interpolated entries: {len(ais_interpolated)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349786ea-439f-4d4d-8b64-12d6b86809e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 1141696, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 36.827699\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 1141696, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 11.011643\n",
      "Latitude MAE: 12.1392\n",
      "Latitude MSE: 321.0075\n",
      "Longitude MAE: 38.8305\n",
      "Longitude MSE: 3333.2154\n",
      "Submission file created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Feature engineering\n",
    "def create_features(df):\n",
    "    df['datetime'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['minute'] = df['datetime'].dt.minute\n",
    "    df['elapsed_time'] = df['datetime'].astype(np.int64) // 10**9  # Convert to seconds since epoch\n",
    "    return df\n",
    "\n",
    "# Load your datasets\n",
    "# ais_train = pd.read_csv('ais_train.csv')\n",
    "# ais_test = pd.read_csv('ais_test.csv')\n",
    "\n",
    "# Apply feature engineering\n",
    "ais_train = create_features(ais_train)\n",
    "ais_test = create_features(ais_test)\n",
    "\n",
    "# Convert vesselId to integer\n",
    "ais_train['vesselId'] = ais_train['vesselId'].astype(int)\n",
    "ais_test['vesselId'] = ais_test['vesselId'].astype(int)\n",
    "\n",
    "# Define features\n",
    "features = ['vesselId', 'elapsed_time']\n",
    "\n",
    "# Prepare training data\n",
    "X = ais_train[features]\n",
    "y_lat = ais_train['latitude']\n",
    "y_lon = ais_train['longitude']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val, y_train_lat, y_val_lat, y_train_lon, y_val_lon = train_test_split(X, y_lat, y_lon, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LightGBM models for latitude and longitude\n",
    "lat_model = LGBMRegressor()\n",
    "lon_model = LGBMRegressor()\n",
    "\n",
    "lat_model.fit(X_train, y_train_lat)\n",
    "lon_model.fit(X_train, y_train_lon)\n",
    "\n",
    "# Predict on validation set to evaluate the model\n",
    "lat_val_pred = lat_model.predict(X_val)\n",
    "lon_val_pred = lon_model.predict(X_val)\n",
    "\n",
    "# Calculate MAE and MSE for latitude\n",
    "mae_lat = mean_absolute_error(y_val_lat, lat_val_pred)\n",
    "mse_lat = mean_squared_error(y_val_lat, lat_val_pred)\n",
    "\n",
    "# Calculate MAE and MSE for longitude\n",
    "mae_lon = mean_absolute_error(y_val_lon, lon_val_pred)\n",
    "mse_lon = mean_squared_error(y_val_lon, lon_val_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Latitude MAE: {mae_lat:.4f}')\n",
    "print(f'Latitude MSE: {mse_lat:.4f}')\n",
    "print(f'Longitude MAE: {mae_lon:.4f}')\n",
    "print(f'Longitude MSE: {mse_lon:.4f}')\n",
    "\n",
    "# Prepare the test data\n",
    "X_test = ais_test[features]\n",
    "\n",
    "# Make predictions on the test set\n",
    "lat_test_pred = lat_model.predict(X_test)\n",
    "lon_test_pred = lon_model.predict(X_test)\n",
    "\n",
    "# Create the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'ID': ais_test['ID'],\n",
    "    'longitude_predicted': lon_test_pred,\n",
    "    'latitude_predicted': lat_test_pred\n",
    "})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
