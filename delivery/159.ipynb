{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710d0f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from darts import TimeSeries\n",
    "from darts.models import LightGBMModel\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ais_train.csv with separator '|'\n",
    "train_df = pd.read_csv('ais_train.csv', sep='|')\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ais_test.csv with separator ','\n",
    "test_df = pd.read_csv('ais_test.csv', sep=',')\n",
    "test_df['time'] = pd.to_datetime(test_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5159d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'vesselId' instead of 'vessel_id'\n",
    "# Select only vessel IDs that are in both train and test datasets\n",
    "common_vessel_ids = set(train_df['vesselId']).intersection(set(test_df['vesselId']))\n",
    "train_df = train_df[train_df['vesselId'].isin(common_vessel_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497eab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the training data by vesselId\n",
    "groups = train_df.groupby('vesselId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store TimeSeries objects and last training times\n",
    "timeseries_dict = {}\n",
    "last_train_time = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea208a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each vesselId group\n",
    "for vessel_id, group_df in groups:\n",
    "    # Sort on time\n",
    "    group_df = group_df.sort_values('time')\n",
    "    # Set index to time\n",
    "    group_df = group_df.set_index('time')\n",
    "    # Select features (latitude and longitude)\n",
    "    features_df = group_df[['latitude', 'longitude']]\n",
    "    # Resample data to hourly frequency with mean and linear interpolation\n",
    "    features_df = features_df.resample('h').mean().interpolate(method='cubic')\n",
    "    # Create Darts TimeSeries object\n",
    "    ts = TimeSeries.from_dataframe(features_df, value_cols=['latitude', 'longitude'])\n",
    "    # Store the TimeSeries object and last training time\n",
    "    timeseries_dict[vessel_id] = ts\n",
    "    last_train_time[vessel_id] = features_df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fa0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store predictions\n",
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6bbdb9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fit LightGBM models and predict for each TimeSeries object\n",
    "for vessel_id, ts in timeseries_dict.items():\n",
    "    # Get the last training time\n",
    "    last_time = last_train_time[vessel_id]\n",
    "    # Get test times for this vessel\n",
    "    vessel_test_df = test_df[test_df['vesselId'] == vessel_id]\n",
    "    test_times = vessel_test_df['time']\n",
    "    # Compute the time differences in hours\n",
    "    time_diffs = (test_times - last_time).dt.total_seconds() / 3600\n",
    "    # Get the maximum forecast horizon needed\n",
    "    max_n = int(np.ceil(time_diffs.max()))\n",
    "    if max_n <= 0:\n",
    "        continue  # Skip if no future times to predict\n",
    "    # Initialize LightGBM model with lag parameters\n",
    "    model = LightGBMModel(\n",
    "        lags=48, # the correct value is between 48 and 96\n",
    "        learning_rate=0.1,\n",
    "        #n_estimators=100,\n",
    "        #max_depth=10,\n",
    "    )\n",
    "    # Fit the model\n",
    "    model.fit(ts)\n",
    "    # Predict up to the maximum horizon needed\n",
    "    forecast = model.predict(max_n)\n",
    "    # Store the forecast and last time\n",
    "    predictions[vessel_id] = (forecast, last_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b03dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store submission rows\n",
    "submission_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7101a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the submission file\n",
    "for idx, row in test_df.iterrows():\n",
    "    vessel_id = row['vesselId']\n",
    "    test_time = row['time']\n",
    "    test_id = row['ID']  # Assuming 'ID' column exists in test_df\n",
    "    # Check if predictions are available for this vessel_id\n",
    "    if vessel_id in predictions:\n",
    "        forecast_ts, last_time = predictions[vessel_id]\n",
    "        time_diff = (test_time - last_time).total_seconds() / 3600\n",
    "        index = int(np.round(time_diff)) - 1  # Adjust index since forecast starts from last_time + 1 hour\n",
    "        # Convert forecast_ts to DataFrame\n",
    "        forecast_df = forecast_ts.pd_dataframe()\n",
    "        # Check if index is within forecast horizon\n",
    "        if 0 <= index < len(forecast_df):\n",
    "            predicted_lat = forecast_df['latitude'].iloc[index]\n",
    "            predicted_lon = forecast_df['longitude'].iloc[index]\n",
    "        else:\n",
    "            predicted_lat = np.nan\n",
    "            predicted_lon = np.nan\n",
    "    else:\n",
    "        predicted_lat = np.nan\n",
    "        predicted_lon = np.nan\n",
    "    # Append the prediction to the submission list\n",
    "    submission_rows.append({\n",
    "        'ID': test_id,\n",
    "        'longitude_predicted': predicted_lon,\n",
    "        'latitude_predicted': predicted_lat\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67080cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission DataFrame from the list\n",
    "submission_df = pd.DataFrame(submission_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
