{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0db9005-4cb9-4d12-8bdb-406a4b9881e6",
   "metadata": {},
   "source": [
    "# Vallhall Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb7af9b-3fef-4177-b86a-38824656eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "from typing import Tuple, Dict\n",
    "from colorlog import ColoredFormatter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Bidirectional, GRU, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rtree import index\n",
    "from shapely.geometry import Point\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb854ab4-aec1-4424-b793-e8c602afd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the colorful logger\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a colorful logger for the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"ML_Pipeline\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Define log colors for different levels\n",
    "    formatter = ColoredFormatter(\n",
    "        \"%(log_color)s%(levelname)-8s%(reset)s | %(log_color)s%(message)s%(reset)s\",\n",
    "        datefmt=None,\n",
    "        log_colors={\n",
    "            'DEBUG':    'cyan',\n",
    "            'INFO':     'white',\n",
    "            'WARNING':  'yellow',\n",
    "            'ERROR':    'red',\n",
    "            'CRITICAL': 'bold_red',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Stream handler for console output\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize the logger\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab026f-ad8e-49d4-9527-13a394875814",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a14a9d5-0362-4adc-9267-9ee1e7b1204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mLoading datasets.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mDatasets loaded successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading datasets.\")\n",
    "ais_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "ais_test = pd.read_csv('ais_test.csv')\n",
    "vessels = pd.read_csv('vessels.csv', sep='|')\n",
    "ports = pd.read_csv('ports.csv', sep='|')\n",
    "schedules = pd.read_csv('schedules_to_may_2024.csv', sep='|', on_bad_lines='skip')\n",
    "logger.info(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e8193-5617-4f06-b4d8-39d26767d576",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b1c0f5-53d4-46e7-9054-914ea5bed0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n",
      "1522065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating Missing Days For Vessels: 100%|██████████████████████████| 711/711 [02:11<00:00,  5.39vessel/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558574\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mTime to build R-tree index for ports: 0.04 seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating nearest ports: 100%|████████████████████████████| 1558574/1558574 [02:24<00:00, 10822.27vessel/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mTime to calculate closest port for all vessels: 144.02 seconds for 1558574 vessels\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding y 5 days forward: 100%|████████████████████████████████████████| 711/711 [05:57<00:00,  1.99vessel/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mData preparation complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Prepare the data for the LSTM model, including additional time-based, vessel-specific, and port proximity features.\n",
    "\n",
    "Args:\n",
    "    ais_train: DataFrame containing AIS training data.\n",
    "    \n",
    "Returns:\n",
    "    Tuple containing the feature array (X), target array (y), and the fitted scaler.\n",
    "\"\"\"\n",
    "logger.info(\"Preparing data for the model.\")\n",
    "\n",
    "vessel_ids = {}\n",
    "i = 0\n",
    "\n",
    "# Use iterrows to iterate through the DataFrame\n",
    "for _, row in vessels.iterrows():\n",
    "    vessel_id = row[\"vesselId\"]\n",
    "    if vessel_id not in vessel_ids:\n",
    "        vessel_ids[vessel_id] = i\n",
    "        i += 1\n",
    "\n",
    "# Convert the 'time' column to datetime format for feature extraction\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "for col in ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'etaRaw']:\n",
    "    ais_train[col] = pd.to_numeric(ais_train[col], errors='coerce')\n",
    "\n",
    "# Clip latitude and longitude to their valid ranges\n",
    "ais_train['latitude'] = ais_train['latitude'].clip(-90, 90)\n",
    "ais_train['longitude'] = ais_train['longitude'].clip(-180, 180)\n",
    "\n",
    "print(len(ais_train))\n",
    "\n",
    "# List to store the interpolated data points\n",
    "interpolated_data = []\n",
    "\n",
    "# Loop through each vessel ID\n",
    "for vessel_id in tqdm(vessel_ids.keys(), desc=\"Interpolating Missing Days For Vessels\", unit=\"vessel\"):\n",
    "    # Filter and sort the data for the current vessel ID\n",
    "    vessel_data = ais_train[ais_train['vesselId'] == vessel_id].sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "    # Proceed only if vessel_data is not empty\n",
    "    if len(vessel_data) > 0:\n",
    "        # Loop through the sorted data to check time differences\n",
    "        for i in range(len(vessel_data) - 1):\n",
    "            current_row = vessel_data.iloc[i]\n",
    "            next_row = vessel_data.iloc[i + 1]\n",
    "\n",
    "            time_difference = next_row['time'] - current_row['time']\n",
    "            \n",
    "            # Add the current row to the interpolated data list\n",
    "            interpolated_data.append(current_row)\n",
    "\n",
    "            # Check if the time difference is greater than 1 day\n",
    "            if time_difference > pd.Timedelta(days=1):\n",
    "                # Calculate the number of missing days\n",
    "                num_missing_days = (time_difference.days - 1)\n",
    "\n",
    "                # Linearly interpolate values for each missing day\n",
    "                for day in range(1, num_missing_days + 1):\n",
    "                    interpolated_time = current_row['time'] + pd.Timedelta(days=day)\n",
    "                    \n",
    "                    # Interpolate all relevant columns\n",
    "                    interpolated_values = {}\n",
    "                    for col in ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'etaRaw']:\n",
    "                        value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
    "                        interpolated_values[col] = current_row[col] + value_diff * day\n",
    "                    \n",
    "                    # Create a new interpolated data point\n",
    "                    interpolated_point = current_row.copy()\n",
    "                    interpolated_point['time'] = interpolated_time\n",
    "                    interpolated_point['latitude'] = interpolated_values['latitude']\n",
    "                    interpolated_point['longitude'] = interpolated_values['longitude']\n",
    "                    interpolated_point['cog'] = interpolated_values['cog']\n",
    "                    interpolated_point['sog'] = interpolated_values['sog']\n",
    "                    interpolated_point['rot'] = interpolated_values['rot']\n",
    "                    interpolated_point['heading'] = interpolated_values['heading']\n",
    "                    interpolated_point['etaRaw'] = interpolated_values['etaRaw']\n",
    "\n",
    "                    # Copy the values of 'vesselId', 'portId', and 'navstat' directly from the current row\n",
    "                    interpolated_point['vesselId'] = current_row['vesselId']\n",
    "                    interpolated_point['portId'] = current_row['portId']\n",
    "                    interpolated_point['navstat'] = current_row['navstat']\n",
    "\n",
    "                    # Add the interpolated point to the list\n",
    "                    interpolated_data.append(interpolated_point)\n",
    "\n",
    "        # Add the last row to the interpolated data list\n",
    "        interpolated_data.append(vessel_data.iloc[-1])\n",
    "# Convert the list of interpolated data back into a DataFrame\n",
    "interpolated_df = pd.DataFrame(interpolated_data)\n",
    "\n",
    "# Combine the interpolated data with the original ais_train DataFrame\n",
    "combined_df = pd.concat([ais_train, interpolated_df]).drop_duplicates().sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "ais_train = combined_df\n",
    "\n",
    "print(len(ais_train))\n",
    "\n",
    "# Extract hour of the day and day of the week as new features\n",
    "ais_train['hour_of_day'] = ais_train['time'].dt.hour\n",
    "ais_train['day_of_week'] = ais_train['time'].dt.dayofweek\n",
    "\n",
    "# Calculate the time elapsed since the first recorded entry for each vessel\n",
    "ais_train['time_elapsed'] = (ais_train['time'] - ais_train['time'].min()).dt.total_seconds()\n",
    "\n",
    "# Map vesselId to its encoded value using vessel_ids dictionary\n",
    "ais_train['vesselId_encoded'] = ais_train['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "\n",
    "# Compute the sine and cosine of the course over ground (cog) to represent direction\n",
    "ais_train['cog_sin'] = np.sin(np.deg2rad(ais_train['cog']))\n",
    "ais_train['cog_cos'] = np.cos(np.deg2rad(ais_train['cog']))\n",
    "\n",
    "# Categorize the speed of the vessel\n",
    "ais_train['speed_category'] = pd.cut(ais_train['sog'], bins=[-1, 5, 15, np.inf], labels=[0, 1, 2])\n",
    "\n",
    "# Merge vessel-specific information into ais_train data\n",
    "ais_train = ais_train.merge(vessels[['vesselId', 'maxSpeed', 'length', 'yearBuilt']], on='vesselId', how='left')\n",
    "\n",
    "# Fill missing values in vessel-specific data with appropriate defaults\n",
    "ais_train['maxSpeed'] = ais_train['maxSpeed'].fillna(ais_train['maxSpeed'].mean())\n",
    "\n",
    "# Measure time to create the R-tree index for ports\n",
    "start_time = time.time()\n",
    "port_idx = index.Index()\n",
    "for idx, row in ports.iterrows():\n",
    "    port_idx.insert(idx, (row['longitude'], row['latitude'], row['longitude'], row['latitude']))\n",
    "end_time = time.time()\n",
    "logger.info(f\"Time to build R-tree index for ports: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure time to calculate the nearest port for each vessel with progress and ETA\n",
    "total_vessels = len(ais_train)\n",
    "start_time = time.time()\n",
    "closest_ports = []\n",
    "\n",
    "for _, vessel in tqdm(ais_train.iterrows(), total=total_vessels, desc=\"Calculating nearest ports\", unit=\"vessel\"):\n",
    "    point = Point(vessel['longitude'], vessel['latitude'])\n",
    "    nearest_port_idx = list(port_idx.nearest((vessel['longitude'], vessel['latitude'], vessel['longitude'], vessel['latitude']), 1))[0]\n",
    "    closest_port = ports.iloc[nearest_port_idx]\n",
    "    distance_to_port = point.distance(Point(closest_port['longitude'], closest_port['latitude']))\n",
    "    closest_ports.append(distance_to_port)\n",
    "\n",
    "end_time = time.time()\n",
    "logger.info(f\"Time to calculate closest port for all vessels: {end_time - start_time:.2f} seconds for {total_vessels} vessels\")\n",
    "\n",
    "# Add the calculated distances to the AIS data\n",
    "ais_train['distance_to_nearest_port'] = closest_ports\n",
    "\n",
    "speed_threshold = 5.0  # knots\n",
    "distance_threshold = 1.0  # kilometers\n",
    "\n",
    "# Identify if the vessel is anchored\n",
    "ais_train['anchored'] = (ais_train['sog'] < speed_threshold) & (ais_train['distance_to_nearest_port'] < distance_threshold)\n",
    "\n",
    "\n",
    "# Extract the relevant features, including the new ones\n",
    "features = ais_train[['latitude', 'longitude', 'sog', 'day_of_week', 'distance_to_nearest_port', 'anchored', 'vesselId_encoded', 'time_elapsed']].values\n",
    "target = ais_train[['latitude', 'longitude']].shift(-1).ffill().values\n",
    "\n",
    "# Normalize features\n",
    "feature_scaler = MinMaxScaler()\n",
    "features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "# Prepare the features and target values for each vessel separately\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Iterate over each vessel ID in the dataset\n",
    "for vessel_id in tqdm(vessel_ids.keys(), total=len(vessel_ids), desc=\"Finding y 5 days forward\", unit=\"vessel\"):\n",
    "    # Filter data for the current vessel and sort it by time\n",
    "    vessel_data = ais_train[ais_train['vesselId'] == vessel_id].reset_index(drop=True)\n",
    "\n",
    "    # Ensure that we have at least 5 days of data for this vessel\n",
    "    for i in range(len(vessel_data) - 5):\n",
    "        # Append the feature set (X) for the current day\n",
    "        X_list.append(vessel_data.iloc[i].values)\n",
    "\n",
    "        # Append the target set (y) which is the position 5 days later\n",
    "        y_list.append(vessel_data.iloc[i + 5][['latitude', 'longitude']].values)\n",
    "\n",
    "# Convert the collected lists to numpy arrays\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "# Reshape for LSTM input: (samples, timesteps, features)\n",
    "X = features_scaled.reshape((features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
    "\n",
    "# Normalize target data (latitude and longitude)\n",
    "target_scaler = MinMaxScaler()\n",
    "y = target_scaler.fit_transform(target)    \n",
    "\n",
    "logger.info(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfcc7b-525a-4510-8c76-dc1ad248286d",
   "metadata": {},
   "source": [
    "## Haversine Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd3b7c9-9eee-4014-9db7-33996d272a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodesic_loss(y_true, y_pred):\n",
    "    \"\"\"Calculate the Haversine distance between true and predicted coordinates.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor of true coordinates (latitude, longitude).\n",
    "        y_pred: Tensor of predicted coordinates (latitude, longitude).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor representing the geodesic distance (Haversine distance) between the true and predicted points.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Split the latitude and longitude into separate tensors\n",
    "    lat_true, lon_true = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
    "    lat_pred, lon_pred = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "    \n",
    "    # Convert degrees to radians manually\n",
    "    lat_true = lat_true * tf.constant(np.pi / 180.0)\n",
    "    lon_true = lon_true * tf.constant(np.pi / 180.0)\n",
    "    lat_pred = lat_pred * tf.constant(np.pi / 180.0)\n",
    "    lon_pred = lon_pred * tf.constant(np.pi / 180.0)\n",
    "    \n",
    "    # Compute the differences between true and predicted coordinates\n",
    "    dlat = lat_pred - lat_true\n",
    "    dlon = lon_pred - lon_true\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = tf.square(tf.sin(dlat / 2)) + tf.cos(lat_true) * tf.cos(lat_pred) * tf.square(tf.sin(dlon / 2))\n",
    "    c = 2 * tf.atan2(tf.sqrt(a), tf.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return tf.reduce_mean(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67001d7a-6fda-426b-b142-945419f1ce13",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "986f89af-9186-4a79-931a-4f3da6ab6114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build the LSTM model.\n",
    "\n",
    "Args:\n",
    "    input_shape: Shape of the input data (timesteps, features).\n",
    "    \n",
    "Returns:\n",
    "    Compiled LSTM model.\n",
    "\"\"\"\n",
    "logger.info(\"Building the LSTM model.\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X.shape[1], X.shape[2])))  # Use Input layer to specify the shape\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=2))  # Output: latitude and longitude\n",
    "model.compile(optimizer='adam', loss=geodesic_loss)\n",
    "logger.info(\"Model built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea15563-5c80-4771-8237-03e9d2afafd4",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e57aad7-a182-4ee8-afc0-7d00d3280413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "Epoch 1/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 873us/step - loss: 5.3864 - val_loss: 0.7025 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 857us/step - loss: 2.9127 - val_loss: 0.9240 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 874us/step - loss: 2.8501 - val_loss: 0.9182 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 858us/step - loss: 2.7942 - val_loss: 1.1854 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 862us/step - loss: 2.7270 - val_loss: 0.6904 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 873us/step - loss: 2.7182 - val_loss: 0.5227 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 879us/step - loss: 2.7048 - val_loss: 0.8398 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 875us/step - loss: 2.6774 - val_loss: 0.7342 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 875us/step - loss: 2.6601 - val_loss: 0.4347 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 902us/step - loss: 2.6457 - val_loss: 0.4787 - learning_rate: 5.0000e-04\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train the LSTM model.\n",
    "\n",
    "Args:\n",
    "    model: The LSTM model to train.\n",
    "    X_train: Training features.\n",
    "    y_train: Training targets.\n",
    "\"\"\"\n",
    "logger.info(\"Starting model training.\")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2,callbacks=[early_stopping, reduce_lr])\n",
    "logger.info(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b990d-4d8c-4abc-b74e-312aced105db",
   "metadata": {},
   "source": [
    "## Generating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2329825-660d-4058-9f4c-6b6aff36b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate a submission file with the predicted vessel positions.\n",
    "\n",
    "Args:\n",
    "    model: Trained LSTM model.\n",
    "    ais_test: DataFrame containing AIS test data.\n",
    "    feature_scaler: Scaler used to normalize the features.\n",
    "    target_scaler: Scaler used to normalize the target coordinates.\n",
    "    vessel_ids: Dictionary to map vessel IDs to numerical indices.\n",
    "\"\"\"\n",
    "logger.info(\"Generating predictions for the test set.\")\n",
    "\n",
    "# Convert the 'time' column to datetime format to handle arithmetic operations\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'], errors='coerce')\n",
    "\n",
    "# Map vesselId to its encoded value using vessel_ids dictionary\n",
    "ais_test['vesselId_encoded'] = ais_test['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "\n",
    "# Calculate the time elapsed since the first recorded entry for each vessel\n",
    "ais_test['time_elapsed'] = (ais_test['time'] - ais_test['time'].min()).dt.total_seconds()\n",
    "\n",
    "# Extract the relevant features for the test data\n",
    "test_features = ais_test[['vesselId_encoded', 'time_elapsed']].values\n",
    "\n",
    "# Since the test data only has one feature, we need to adjust the input shape to match the model's expectation\n",
    "num_train_features = feature_scaler.n_features_in_\n",
    "test_features_padded = np.zeros((test_features.shape[0], num_train_features))\n",
    "test_features_padded[:, :test_features.shape[1]] = test_features\n",
    "\n",
    "# Normalize the padded test features to match the training data scale\n",
    "test_features_scaled = feature_scaler.transform(test_features_padded)\n",
    "X_test = test_features_scaled.reshape((test_features_scaled.shape[0], 1, test_features_scaled.shape[1]))\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions using the target scaler\n",
    "predictions = target_scaler.inverse_transform(predictions)\n",
    "\n",
    "# Create the submission DataFrame in the required format\n",
    "submission = pd.DataFrame({\n",
    "    'ID': ais_test['ID'],\n",
    "    'longitude_predicted': predictions[:, 1],\n",
    "    'latitude_predicted': predictions[:, 0]\n",
    "})\n",
    "\n",
    "# # Function to wrap latitude values properly\n",
    "# def wrap_latitude(lat):\n",
    "#     # Continue wrapping while latitude is outside the range [-90, 90]\n",
    "#     while lat > 90 or lat < -90:\n",
    "#         if lat > 90:\n",
    "#             lat = 180 - lat  # Reflect latitude if it goes beyond 90\n",
    "#         elif lat < -90:\n",
    "#             lat = -180 - lat  # Reflect latitude if it goes below -90\n",
    "#     return lat\n",
    "#\n",
    "# # Function to wrap longitude values properly\n",
    "# def wrap_longitude(lon):\n",
    "#     return ((lon + 180) % 360) - 180  # Wrap longitude to range [-180, 180]\n",
    "#\n",
    "# # Apply the wrapping to predicted latitude and longitude values\n",
    "# submission['latitude_predicted'] = submission['latitude_predicted'].apply(wrap_latitude)\n",
    "# submission['longitude_predicted'] = submission['longitude_predicted'].apply(wrap_longitude)\n",
    "\n",
    "# Ensure that the submission file has exactly 51739 rows as required\n",
    "assert submission.shape[0] == 51739, \"The submission file must have exactly 51739 rows.\"\n",
    "\n",
    "# Save the predictions to submission.csv\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "logger.info(\"Submission file saved as submission.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84692863-bff4-458a-9b65-c180dc7e8854",
   "metadata": {},
   "source": [
    "# Submission stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6818d7-ac66-4ff8-9c12-ca13614c0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('submission.csv')\n",
    "\n",
    "# Calculate the maximum and minimum values for longitude_predicted and latitude_predicted\n",
    "longitude_max = df['longitude_predicted'].max()\n",
    "longitude_min = df['longitude_predicted'].min()\n",
    "latitude_max = df['latitude_predicted'].max()\n",
    "latitude_min = df['latitude_predicted'].min()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum Longitude Predicted: {longitude_max}\")\n",
    "print(f\"Minimum Longitude Predicted: {longitude_min}\")\n",
    "print(f\"Maximum Latitude Predicted: {latitude_max}\")\n",
    "print(f\"Minimum Latitude Predicted: {latitude_min}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
