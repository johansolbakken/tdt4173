{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3e6b87-9678-4ede-9772-30de20553b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time    cog   sog  rot  heading  navstat       etaRaw  \\\n",
      "0 2024-01-01 00:00:25  284.0   0.7    0       88        0  01-09 23:00   \n",
      "1 2024-01-01 00:00:36  109.6   0.0   -6      347        1  12-29 20:00   \n",
      "2 2024-01-01 00:01:45  111.0  11.0    0      112        0  01-02 09:00   \n",
      "3 2024-01-01 00:03:11   96.4   0.0    0      142        1  12-31 20:00   \n",
      "4 2024-01-01 00:03:51  214.0  19.7    0      215        0  01-25 12:00   \n",
      "\n",
      "   latitude  longitude                  vesselId  ... hour_of_day_14  \\\n",
      "0 -34.74370  -57.85130  61e9f3a8b937134a3c4bfdf7  ...          False   \n",
      "1   8.89440  -79.47939  61e9f3d4b937134a3c4bff1f  ...          False   \n",
      "2  39.19065  -76.47567  61e9f436b937134a3c4c0131  ...          False   \n",
      "3 -34.41189  151.02067  61e9f3b4b937134a3c4bfe77  ...          False   \n",
      "4  35.88379   -5.91636  61e9f41bb937134a3c4c0087  ...          False   \n",
      "\n",
      "   hour_of_day_15  hour_of_day_16  hour_of_day_17  hour_of_day_18  \\\n",
      "0           False           False           False           False   \n",
      "1           False           False           False           False   \n",
      "2           False           False           False           False   \n",
      "3           False           False           False           False   \n",
      "4           False           False           False           False   \n",
      "\n",
      "   hour_of_day_19  hour_of_day_20  hour_of_day_21  hour_of_day_22  \\\n",
      "0           False           False           False           False   \n",
      "1           False           False           False           False   \n",
      "2           False           False           False           False   \n",
      "3           False           False           False           False   \n",
      "4           False           False           False           False   \n",
      "\n",
      "   hour_of_day_23  \n",
      "0           False  \n",
      "1           False  \n",
      "2           False  \n",
      "3           False  \n",
      "4           False  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "nihao\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johansolbakken/miniforge3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 9ms/step - loss: 0.0045 - val_loss: 2.5583e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 9ms/step - loss: 9.2231e-04 - val_loss: 2.9519e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 8ms/step - loss: 8.6831e-04 - val_loss: 3.4145e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 9ms/step - loss: 8.5906e-04 - val_loss: 2.4289e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 9ms/step - loss: 8.4516e-04 - val_loss: 2.3921e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 9ms/step - loss: 8.4418e-04 - val_loss: 2.4918e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 9ms/step - loss: 8.3263e-04 - val_loss: 2.5516e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 9ms/step - loss: 8.2322e-04 - val_loss: 2.5049e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 9ms/step - loss: 8.1724e-04 - val_loss: 2.3082e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 9ms/step - loss: 8.0817e-04 - val_loss: 2.2924e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 9ms/step - loss: 8.1914e-04 - val_loss: 2.5153e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 9ms/step - loss: 8.1903e-04 - val_loss: 2.3603e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 9ms/step - loss: 8.0765e-04 - val_loss: 2.3405e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 8ms/step - loss: 8.0622e-04 - val_loss: 2.3392e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 9ms/step - loss: 8.0601e-04 - val_loss: 2.4902e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_95859/1036829083.py:138: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_positions = ais_train_scaled.groupby('vesselId').apply(lambda x: x.sort_values('elapsed_time').tail(time_step))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1617/1617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "   ID  longitude_predicted  latitude_predicted\n",
      "0   0           -84.068459           31.791695\n",
      "1   1           122.680740           14.680503\n",
      "2   2            10.316170           37.626579\n",
      "3   3           175.012146          -40.195789\n",
      "4   4            -5.778167           49.089722\n",
      "Submission DataFrame shape: (51739, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\"\"\"\n",
    "    Load and Preprocess Data\n",
    "\"\"\"\n",
    "\n",
    "# Read ais_train.csv\n",
    "ais_train = pd.read_csv(\"ais_train.csv\", sep='|')\n",
    "\n",
    "# Temporal features\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "ais_train['elapsed_time'] = (ais_train['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "ais_train['day_of_week'] = ais_train['time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "ais_train['hour_of_day'] = ais_train['time'].dt.hour\n",
    "ais_train = pd.get_dummies(ais_train, columns=['day_of_week', 'hour_of_day'], drop_first=True)\n",
    "\n",
    "# Filter out unrealistic speeds\n",
    "ais_train = ais_train[ais_train['sog'] < 25]\n",
    "\n",
    "# Map 'navstat' values\n",
    "ais_train['navstat'] = ais_train['navstat'].replace(8, 0)  # Under way sailing -> Under way using engine\n",
    "ais_train = ais_train[~((ais_train['navstat'].isin([1, 5])) & (ais_train['sog'] > 0))]\n",
    "ais_train = ais_train[~((ais_train['navstat'] == 2) & (ais_train['sog'] > 5))]\n",
    "\n",
    "# One-hot encode 'navstat'\n",
    "ais_train = pd.get_dummies(ais_train, columns=['navstat'])\n",
    "\n",
    "# Merge with vessel data\n",
    "vessels = pd.read_csv(\"vessels.csv\", sep='|')[['shippingLineId', 'vesselId']]\n",
    "vessels['new_id'] = range(len(vessels))\n",
    "vessel_id_to_new_id = dict(zip(vessels['vesselId'], vessels['new_id']))\n",
    "ais_train = pd.merge(ais_train, vessels, on='vesselId', how='left')\n",
    "\n",
    "# Define input and target features\n",
    "input_features = ['latitude', 'longitude', 'sog', 'cog', 'heading', 'elapsed_time']\n",
    "input_features.extend([col for col in ais_train.columns if 'day_of_week_' in col])\n",
    "input_features.extend([col for col in ais_train.columns if 'hour_of_day_' in col])\n",
    "navstat_columns = [col for col in ais_train.columns if col.startswith('navstat_')]\n",
    "input_features.extend(navstat_columns)\n",
    "target_columns = ['latitude', 'longitude']\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_input = MinMaxScaler()\n",
    "scaler_output = MinMaxScaler()\n",
    "\n",
    "# Scale input and output features\n",
    "input_data = scaler_input.fit_transform(ais_train[input_features])\n",
    "output_data = scaler_output.fit_transform(ais_train[target_columns])\n",
    "\n",
    "# Add scaled features back to DataFrame\n",
    "ais_train_scaled = ais_train.copy()\n",
    "ais_train_scaled[input_features] = input_data\n",
    "ais_train_scaled[target_columns] = output_data\n",
    "\n",
    "# Function to create sequences per vessel\n",
    "def create_sequences_per_vessel(df, time_steps):\n",
    "    X, y = [], []\n",
    "    vessel_ids = df['vesselId'].unique()\n",
    "    for vessel_id in vessel_ids:\n",
    "        vessel_data = df[df['vesselId'] == vessel_id].sort_values('elapsed_time')\n",
    "        inputs = vessel_data[input_features].values\n",
    "        targets = vessel_data[target_columns].values\n",
    "        if len(inputs) < time_steps:\n",
    "            continue  # Skip sequences shorter than time_steps\n",
    "        for i in range(len(inputs) - time_steps):\n",
    "            X.append(inputs[i:i + time_steps])\n",
    "            y.append(targets[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "time_step = 10\n",
    "X, y = create_sequences_per_vessel(ais_train_scaled, time_step)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "\"\"\"\n",
    "    Define and Train the Model\n",
    "\"\"\"\n",
    "\n",
    "# Define the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(time_step, X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    Prepare Test Data and Make Predictions\n",
    "\"\"\"\n",
    "\n",
    "# Load test data\n",
    "ais_test = pd.read_csv(\"ais_test.csv\")\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'])\n",
    "ais_test['elapsed_time'] = (ais_test['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "ais_test['new_id'] = ais_test['vesselId'].map(vessel_id_to_new_id)\n",
    "\n",
    "ais_test['day_of_week'] = ais_test['time'].dt.dayofweek\n",
    "ais_test['hour_of_day'] = ais_test['time'].dt.hour\n",
    "\n",
    "# One-hot encode\n",
    "ais_test = pd.get_dummies(ais_test, columns=['day_of_week', 'hour_of_day'], drop_first=True)\n",
    "\n",
    "# Ensure all columns in ais_test match those in ais_train\n",
    "for col in input_features:\n",
    "    if col not in ais_test.columns:\n",
    "        ais_test[col] = 0\n",
    "\n",
    "# One-hot encode 'navstat' in test data (if available)\n",
    "# If 'navstat' is not available in test data, you may need to handle this accordingly\n",
    "\n",
    "# Merge with last known positions from training data\n",
    "# Get the last 'time_step' records for each vessel from training data\n",
    "last_positions = ais_train_scaled.groupby('vesselId').apply(lambda x: x.sort_values('elapsed_time').tail(time_step))\n",
    "last_positions = last_positions.reset_index(drop=True)\n",
    "\n",
    "# Prepare sequences for each vessel in the test set\n",
    "vessel_sequences = {}\n",
    "for vessel_id in ais_test['vesselId'].unique():\n",
    "    if vessel_id in last_positions['vesselId'].values:\n",
    "        vessel_data = last_positions[last_positions['vesselId'] == vessel_id]\n",
    "        seq = vessel_data[input_features].values\n",
    "        if len(seq) < time_step:\n",
    "            # Pad sequences if necessary\n",
    "            seq = np.pad(seq, ((time_step - len(seq), 0), (0, 0)), mode='constant')\n",
    "        vessel_sequences[vessel_id] = seq\n",
    "    else:\n",
    "        # If no data available, create a default sequence (e.g., zeros or mean values)\n",
    "        seq = np.zeros((time_step, len(input_features)))\n",
    "        vessel_sequences[vessel_id] = seq\n",
    "\n",
    "# Create test sequences\n",
    "X_test = []\n",
    "for idx, row in ais_test.iterrows():\n",
    "    vessel_id = row['vesselId']\n",
    "    seq = vessel_sequences[vessel_id]\n",
    "    X_test.append(seq)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_pred_inverse = scaler_output.inverse_transform(y_pred)\n",
    "\n",
    "\"\"\"\n",
    "    Prepare Submission File\n",
    "\"\"\"\n",
    "\n",
    "# Prepare submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': ais_test['ID'].values,\n",
    "    'longitude_predicted': y_pred_inverse[:, target_columns.index('longitude')],\n",
    "    'latitude_predicted': y_pred_inverse[:, target_columns.index('latitude')]\n",
    "})\n",
    "\n",
    "# Ensure the submission file has the required columns\n",
    "submission_df = submission_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# Display submission\n",
    "print(submission_df.head())\n",
    "print(f\"Submission DataFrame shape: {submission_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
