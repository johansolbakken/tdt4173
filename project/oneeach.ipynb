{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3e6b87-9678-4ede-9772-30de20553b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johansolbakken/miniforge3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 2.6323e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6ms/step - loss: 8.8203e-04 - val_loss: 2.9042e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 6ms/step - loss: 8.5491e-04 - val_loss: 2.2642e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 6ms/step - loss: 8.3340e-04 - val_loss: 2.3481e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6ms/step - loss: 8.2362e-04 - val_loss: 2.3615e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 6ms/step - loss: 8.1254e-04 - val_loss: 2.5413e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 6ms/step - loss: 8.3622e-04 - val_loss: 2.8646e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m17520/17520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 6ms/step - loss: 8.1681e-04 - val_loss: 2.4724e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_62707/2613444564.py:119: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_positions = ais_train_scaled.groupby('vesselId').apply(lambda x: x.sort_values('elapsed_time').tail(time_step))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1617/1617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "   ID  longitude_predicted  latitude_predicted\n",
      "0   0           -82.688011           30.940355\n",
      "1   1           119.445145           14.739825\n",
      "2   2            11.130962           38.543983\n",
      "3   3           168.906738          -42.034435\n",
      "4   4            -2.105234           48.945454\n",
      "Submission DataFrame shape: (51739, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\"\"\"\n",
    "    Load and Preprocess Data\n",
    "\"\"\"\n",
    "\n",
    "# Read ais_train.csv\n",
    "ais_train = pd.read_csv(\"ais_train.csv\", sep='|')\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "ais_train['elapsed_time'] = (ais_train['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "# Filter out unrealistic speeds\n",
    "ais_train = ais_train[ais_train['sog'] < 25]\n",
    "\n",
    "# Map 'navstat' values\n",
    "ais_train['navstat'] = ais_train['navstat'].replace(8, 0)  # Under way sailing -> Under way using engine\n",
    "ais_train = ais_train[~((ais_train['navstat'].isin([1, 5])) & (ais_train['sog'] > 0))]\n",
    "ais_train = ais_train[~((ais_train['navstat'] == 2) & (ais_train['sog'] > 5))]\n",
    "\n",
    "# One-hot encode 'navstat'\n",
    "ais_train = pd.get_dummies(ais_train, columns=['navstat'])\n",
    "\n",
    "# Merge with vessel data\n",
    "vessels = pd.read_csv(\"vessels.csv\", sep='|')[['shippingLineId', 'vesselId']]\n",
    "vessels['new_id'] = range(len(vessels))\n",
    "vessel_id_to_new_id = dict(zip(vessels['vesselId'], vessels['new_id']))\n",
    "ais_train = pd.merge(ais_train, vessels, on='vesselId', how='left')\n",
    "\n",
    "# Define input and target features\n",
    "input_features = ['latitude', 'longitude', 'sog', 'cog', 'heading', 'elapsed_time']\n",
    "navstat_columns = [col for col in ais_train.columns if col.startswith('navstat_')]\n",
    "input_features.extend(navstat_columns)\n",
    "target_columns = ['latitude', 'longitude']\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_input = MinMaxScaler()\n",
    "scaler_output = MinMaxScaler()\n",
    "\n",
    "# Scale input and output features\n",
    "input_data = scaler_input.fit_transform(ais_train[input_features])\n",
    "output_data = scaler_output.fit_transform(ais_train[target_columns])\n",
    "\n",
    "# Add scaled features back to DataFrame\n",
    "ais_train_scaled = ais_train.copy()\n",
    "ais_train_scaled[input_features] = input_data\n",
    "ais_train_scaled[target_columns] = output_data\n",
    "\n",
    "# Function to create sequences per vessel\n",
    "def create_sequences_per_vessel(df, time_steps):\n",
    "    X, y = [], []\n",
    "    vessel_ids = df['vesselId'].unique()\n",
    "    for vessel_id in vessel_ids:\n",
    "        vessel_data = df[df['vesselId'] == vessel_id].sort_values('elapsed_time')\n",
    "        inputs = vessel_data[input_features].values\n",
    "        targets = vessel_data[target_columns].values\n",
    "        if len(inputs) < time_steps:\n",
    "            continue  # Skip sequences shorter than time_steps\n",
    "        for i in range(len(inputs) - time_steps):\n",
    "            X.append(inputs[i:i + time_steps])\n",
    "            y.append(targets[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "time_step = 10\n",
    "X, y = create_sequences_per_vessel(ais_train_scaled, time_step)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "\"\"\"\n",
    "    Define and Train the Model\n",
    "\"\"\"\n",
    "\n",
    "# Define the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(time_step, X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    Prepare Test Data and Make Predictions\n",
    "\"\"\"\n",
    "\n",
    "# Load test data\n",
    "ais_test = pd.read_csv(\"ais_test.csv\")\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'])\n",
    "ais_test['elapsed_time'] = (ais_test['time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "ais_test['new_id'] = ais_test['vesselId'].map(vessel_id_to_new_id)\n",
    "\n",
    "# One-hot encode 'navstat' in test data (if available)\n",
    "# If 'navstat' is not available in test data, you may need to handle this accordingly\n",
    "\n",
    "# Merge with last known positions from training data\n",
    "# Get the last 'time_step' records for each vessel from training data\n",
    "last_positions = ais_train_scaled.groupby('vesselId').apply(lambda x: x.sort_values('elapsed_time').tail(time_step))\n",
    "last_positions = last_positions.reset_index(drop=True)\n",
    "\n",
    "# Prepare sequences for each vessel in the test set\n",
    "vessel_sequences = {}\n",
    "for vessel_id in ais_test['vesselId'].unique():\n",
    "    if vessel_id in last_positions['vesselId'].values:\n",
    "        vessel_data = last_positions[last_positions['vesselId'] == vessel_id]\n",
    "        seq = vessel_data[input_features].values\n",
    "        if len(seq) < time_step:\n",
    "            # Pad sequences if necessary\n",
    "            seq = np.pad(seq, ((time_step - len(seq), 0), (0, 0)), mode='constant')\n",
    "        vessel_sequences[vessel_id] = seq\n",
    "    else:\n",
    "        # If no data available, create a default sequence (e.g., zeros or mean values)\n",
    "        seq = np.zeros((time_step, len(input_features)))\n",
    "        vessel_sequences[vessel_id] = seq\n",
    "\n",
    "# Create test sequences\n",
    "X_test = []\n",
    "for idx, row in ais_test.iterrows():\n",
    "    vessel_id = row['vesselId']\n",
    "    seq = vessel_sequences[vessel_id]\n",
    "    X_test.append(seq)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_pred_inverse = scaler_output.inverse_transform(y_pred)\n",
    "\n",
    "\"\"\"\n",
    "    Prepare Submission File\n",
    "\"\"\"\n",
    "\n",
    "# Prepare submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': ais_test['ID'].values,\n",
    "    'longitude_predicted': y_pred_inverse[:, target_columns.index('longitude')],\n",
    "    'latitude_predicted': y_pred_inverse[:, target_columns.index('latitude')]\n",
    "})\n",
    "\n",
    "# Ensure the submission file has the required columns\n",
    "submission_df = submission_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# Display submission\n",
    "print(submission_df.head())\n",
    "print(f\"Submission DataFrame shape: {submission_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
