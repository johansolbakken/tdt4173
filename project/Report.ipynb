{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbd0401-0de4-475f-8d4a-27ec0b05d3f5",
   "metadata": {},
   "source": [
    "# First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ac474-7287-47b4-b04a-94a7aa3c77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "import sys\n",
    "from colorlog import ColoredFormatter\n",
    "\n",
    "# Configure the colorful logger\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a colorful logger for the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"ML_Pipeline\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Define log colors for different levels\n",
    "    formatter = ColoredFormatter(\n",
    "        \"%(log_color)s%(levelname)-8s%(reset)s | %(log_color)s%(message)s%(reset)s\",\n",
    "        datefmt=None,\n",
    "        log_colors={\n",
    "            'DEBUG':    'cyan',\n",
    "            'INFO':     'green',\n",
    "            'WARNING':  'yellow',\n",
    "            'ERROR':    'red',\n",
    "            'CRITICAL': 'bold_red',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Stream handler for console output\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize the logger\n",
    "logger = setup_logger()\n",
    "\n",
    "def calculate_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"Calculate the geodesic distance between two latitude-longitude points.\n",
    "\n",
    "    Args:\n",
    "        lat1: Latitude of the first point.\n",
    "        lon1: Longitude of the first point.\n",
    "        lat2: Latitude of the second point.\n",
    "        lon2: Longitude of the second point.\n",
    "\n",
    "    Returns:\n",
    "        The distance in kilometers between the two points.\n",
    "    \"\"\"\n",
    "    distance = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "    logger.debug(f\"Calculated distance: {distance} km between points ({lat1}, {lon1}) and ({lat2}, {lon2})\")\n",
    "    return distance\n",
    "\n",
    "def prepare_sequences(\n",
    "    data: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    target_cols: List[str],\n",
    "    sequence_length: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare input and target sequences for training the LSTM model.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame containing the data.\n",
    "        feature_cols: List of feature column names.\n",
    "        target_cols: List of target column names.\n",
    "        sequence_length: The length of each sequence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the input sequences (X) and target values (y).\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing sequences for training/validation.\")\n",
    "    X, y = [], []\n",
    "    data_values = data[feature_cols + target_cols].values\n",
    "    for i in range(sequence_length, len(data_values) + 1):\n",
    "        X.append(data_values[i - sequence_length:i, :len(feature_cols)])\n",
    "        y.append(data_values[i - 1, len(feature_cols):])\n",
    "    logger.debug(f\"Prepared {len(X)} sequences.\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_test_sequences(\n",
    "    data: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    sequence_length: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Prepare input sequences for the test data.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame containing the test data.\n",
    "        feature_cols: List of feature column names.\n",
    "        sequence_length: The length of each sequence.\n",
    "\n",
    "    Returns:\n",
    "        An array of input sequences for the test data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing sequences for testing.\")\n",
    "    X = []\n",
    "    data_values = data[feature_cols].values\n",
    "    num_samples = len(data_values)\n",
    "    for i in range(num_samples):\n",
    "        start_idx = max(0, i - sequence_length + 1)\n",
    "        seq = data_values[start_idx:i+1]\n",
    "        # Pad sequences that are shorter than the required length\n",
    "        if len(seq) < sequence_length:\n",
    "            padding = np.zeros((sequence_length - len(seq), len(feature_cols)))\n",
    "            seq = np.vstack((padding, seq))\n",
    "            logger.debug(f\"Padded sequence at index {i} with zeros.\")\n",
    "        X.append(seq)\n",
    "    logger.debug(f\"Prepared {len(X)} test sequences.\")\n",
    "    return np.array(X)\n",
    "\n",
    "def prepare_test_data(\n",
    "    ais_test: pd.DataFrame,\n",
    "    vessels: pd.DataFrame,\n",
    "    vessel_type_categories: pd.Index\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Prepare and preprocess the test data.\n",
    "\n",
    "    Args:\n",
    "        ais_test: DataFrame containing the AIS test data.\n",
    "        vessels: DataFrame containing vessel information.\n",
    "        vessel_type_categories: Categories of vessel types from training data.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the merged test DataFrame and the list of feature columns.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing test data.\")\n",
    "    merged_test = ais_test.copy()\n",
    "\n",
    "    # Merge AIS test data with vessel data\n",
    "    merged_test = pd.merge(\n",
    "        merged_test,\n",
    "        vessels[['vesselId', 'vesselType']],\n",
    "        on='vesselId',\n",
    "        how='left'\n",
    "    )\n",
    "    logger.debug(\"Merged AIS test data with vessel data.\")\n",
    "\n",
    "    # Ensure 'vesselType' is of object (string) type and handle missing values\n",
    "    merged_test['vesselType'] = merged_test['vesselType'].astype('object').fillna('Unknown')\n",
    "    logger.debug(\"Handled missing vesselType values.\")\n",
    "\n",
    "    # Extract time-related features\n",
    "    merged_test['time'] = pd.to_datetime(merged_test['time'])\n",
    "    merged_test['hour'] = merged_test['time'].dt.hour\n",
    "    merged_test['day_of_week'] = merged_test['time'].dt.dayofweek\n",
    "    logger.debug(\"Extracted time-related features (hour, day_of_week).\")\n",
    "\n",
    "    # Encode vessel type using categories from training data\n",
    "    merged_test['vesselType'] = pd.Categorical(\n",
    "        merged_test['vesselType'],\n",
    "        categories=vessel_type_categories\n",
    "    )\n",
    "    merged_test['vessel_type_encoded'] = merged_test['vesselType'].cat.codes\n",
    "    logger.debug(\"Encoded vesselType as numeric categories.\")\n",
    "\n",
    "    # Define the feature columns\n",
    "    features = ['hour', 'day_of_week', 'vessel_type_encoded']\n",
    "\n",
    "    # Handle missing feature values\n",
    "    merged_test[features] = merged_test[features].fillna(0)\n",
    "    logger.debug(\"Handled missing feature values in test data.\")\n",
    "\n",
    "    logger.info(\"Test data preparation completed.\")\n",
    "    return merged_test, features\n",
    "\n",
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load the AIS and auxiliary datasets.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the loaded DataFrames:\n",
    "        (ais_train, ais_test, vessels, ports, schedules).\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading datasets.\")\n",
    "    ais_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "    ais_test = pd.read_csv('ais_test.csv')\n",
    "    vessels = pd.read_csv('vessels.csv', sep='|')\n",
    "    ports = pd.read_csv('ports.csv', sep='|')\n",
    "    schedules = pd.read_csv('schedules_to_may_2024.csv', sep='|')\n",
    "    logger.debug(\"Datasets loaded successfully.\")\n",
    "    return ais_train, ais_test, vessels, ports, schedules\n",
    "\n",
    "def preprocess_data(\n",
    "    ais_train: pd.DataFrame,\n",
    "    vessels: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.Index]:\n",
    "    \"\"\"Preprocess the training data and merge with vessel information.\n",
    "\n",
    "    Args:\n",
    "        ais_train: DataFrame containing the AIS training data.\n",
    "        vessels: DataFrame containing vessel information.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the merged training DataFrame and vessel type categories.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preprocessing training data.\")\n",
    "    # Merge AIS data with vessel data to get vessel types and other info\n",
    "    merged_data = pd.merge(\n",
    "        ais_train,\n",
    "        vessels[['vesselId', 'vesselType']],\n",
    "        on='vesselId',\n",
    "        how='left'\n",
    "    )\n",
    "    logger.debug(\"Merged AIS training data with vessel data.\")\n",
    "\n",
    "    # Ensure 'vesselType' is of object type and handle missing values\n",
    "    merged_data['vesselType'] = merged_data['vesselType'].astype('object').fillna('Unknown')\n",
    "    logger.debug(\"Handled missing vesselType values in training data.\")\n",
    "\n",
    "    # Extract time-related features\n",
    "    merged_data['time'] = pd.to_datetime(merged_data['time'])\n",
    "    merged_data['hour'] = merged_data['time'].dt.hour\n",
    "    merged_data['day_of_week'] = merged_data['time'].dt.dayofweek\n",
    "    logger.debug(\"Extracted time-related features (hour, day_of_week) in training data.\")\n",
    "\n",
    "    # Sort data and create future position columns\n",
    "    merged_data = merged_data.sort_values(by=['vesselId', 'time'])\n",
    "    merged_data['future_latitude'] = merged_data.groupby('vesselId')['latitude'].shift(-1)\n",
    "    merged_data['future_longitude'] = merged_data.groupby('vesselId')['longitude'].shift(-1)\n",
    "    logger.debug(\"Created future latitude and longitude columns.\")\n",
    "\n",
    "    # Drop rows with missing future positions\n",
    "    initial_length = len(merged_data)\n",
    "    merged_data.dropna(subset=['future_latitude', 'future_longitude'], inplace=True)\n",
    "    logger.debug(f\"Dropped {initial_length - len(merged_data)} rows with missing future positions.\")\n",
    "\n",
    "    # Encode vessel type as numeric categories\n",
    "    merged_data['vesselType'] = merged_data['vesselType'].astype('category')\n",
    "    vessel_type_categories = merged_data['vesselType'].cat.categories\n",
    "    merged_data['vessel_type_encoded'] = merged_data['vesselType'].cat.codes\n",
    "    logger.debug(\"Encoded vesselType as numeric categories in training data.\")\n",
    "\n",
    "    logger.info(\"Training data preprocessing completed.\")\n",
    "    return merged_data, vessel_type_categories\n",
    "\n",
    "def split_data(\n",
    "    merged_data: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    target: List[str],\n",
    "    sequence_length: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Split the data into training and validation sets and prepare sequences.\n",
    "\n",
    "    Args:\n",
    "        merged_data: DataFrame containing the preprocessed data.\n",
    "        features: List of feature column names.\n",
    "        target: List of target column names.\n",
    "        sequence_length: The length of each sequence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing training and validation sequences and targets:\n",
    "        (X_train, y_train, X_val, y_val, val_data).\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting data into training and validation sets.\")\n",
    "    # Use early data as training, later data as validation\n",
    "    train_data, val_data = train_test_split(merged_data, test_size=0.2, shuffle=False)\n",
    "    logger.debug(f\"Training data size: {len(train_data)}, Validation data size: {len(val_data)}\")\n",
    "\n",
    "    # Handle missing feature values\n",
    "    train_data[features] = train_data[features].fillna(0)\n",
    "    val_data[features] = val_data[features].fillna(0)\n",
    "    logger.debug(\"Handled missing feature values in training and validation data.\")\n",
    "\n",
    "    # Prepare sequences\n",
    "    X_train, y_train = prepare_sequences(train_data, features, target, sequence_length)\n",
    "    X_val, y_val = prepare_sequences(val_data, features, target, sequence_length)\n",
    "    logger.info(\"Data splitting and sequence preparation completed.\")\n",
    "    return X_train, y_train, X_val, y_val, val_data\n",
    "\n",
    "def build_model(input_shape: Tuple[int, int]) -> tf.keras.Model:\n",
    "    \"\"\"Build and compile the LSTM model.\n",
    "\n",
    "    Args:\n",
    "        input_shape: A tuple representing the input shape (sequence_length, num_features).\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    logger.info(\"Building the LSTM model.\")\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dense(2))  # Predict latitude and longitude\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "    logger.debug(\"LSTM model built and compiled.\")\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "    model: tf.keras.Model,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"Train the LSTM model.\n",
    "\n",
    "    Args:\n",
    "        model: The compiled Keras model.\n",
    "        X_train: Training input sequences.\n",
    "        y_train: Training target values.\n",
    "        X_val: Validation input sequences.\n",
    "        y_val: Validation target values.\n",
    "\n",
    "    Returns:\n",
    "        The history object containing training details.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting model training.\")\n",
    "    history = model.fit(X_train, y_train, epochs=3, validation_data=(X_val, y_val))\n",
    "    logger.info(\"Model training completed.\")\n",
    "    return history\n",
    "\n",
    "def evaluate_model(\n",
    "    model: tf.keras.Model,\n",
    "    X_val: np.ndarray,\n",
    "    val_data: pd.DataFrame,\n",
    "    sequence_length: int\n",
    ") -> float:\n",
    "    \"\"\"Evaluate the model on the validation set and calculate error distance.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model.\n",
    "        X_val: Validation input sequences.\n",
    "        val_data: DataFrame containing the validation data.\n",
    "        sequence_length: The length of each sequence.\n",
    "\n",
    "    Returns:\n",
    "        The mean geodesic error distance on the validation set.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating the model on the validation set.\")\n",
    "    predictions_val = model.predict(X_val)\n",
    "    logger.debug(f\"Number of predictions: {predictions_val.shape[0]}\")\n",
    "    logger.debug(f\"Number of validation samples: {len(val_data) - sequence_length + 1}\")\n",
    "\n",
    "    # Align with predictions\n",
    "    val_data_aligned = val_data.iloc[sequence_length - 1:].copy()\n",
    "    logger.debug(f\"Aligned validation data size: {len(val_data_aligned)}\")\n",
    "\n",
    "    # Assign predictions\n",
    "    val_data_aligned['pred_latitude'] = predictions_val[:, 0]\n",
    "    val_data_aligned['pred_longitude'] = predictions_val[:, 1]\n",
    "    logger.debug(\"Predictions on validation set obtained.\")\n",
    "\n",
    "    # Calculate the error distance for each prediction\n",
    "    val_data_aligned['error_distance'] = val_data_aligned.apply(\n",
    "        lambda row: calculate_distance(\n",
    "            row['future_latitude'],\n",
    "            row['future_longitude'],\n",
    "            row['pred_latitude'],\n",
    "            row['pred_longitude']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    mean_error_distance = val_data_aligned['error_distance'].mean()\n",
    "    logger.info(f\"Mean Geodetic Error on Validation Set: {mean_error_distance:.2f} km\")\n",
    "    return mean_error_distance\n",
    "\n",
    "def prepare_submission(\n",
    "    ais_test: pd.DataFrame,\n",
    "    merged_test: pd.DataFrame,\n",
    "    predictions_test: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Prepare the submission DataFrame.\n",
    "\n",
    "    Args:\n",
    "        ais_test: DataFrame containing the AIS test data.\n",
    "        merged_test: DataFrame containing the merged test data.\n",
    "        predictions_test: Numpy array containing the predicted coordinates.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame ready to be saved as a submission file.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing submission DataFrame.\")\n",
    "    # Align predictions with test data\n",
    "    submission_df = merged_test.copy()\n",
    "    submission_df['longitude_predicted'] = predictions_test[:, 1]\n",
    "    submission_df['latitude_predicted'] = predictions_test[:, 0]\n",
    "    logger.debug(\"Added predicted coordinates to submission DataFrame.\")\n",
    "\n",
    "    # Prepare submission file\n",
    "    if 'ID' not in ais_test.columns:\n",
    "        ais_test.reset_index(inplace=True)\n",
    "        ais_test.rename(columns={'index': 'ID'}, inplace=True)\n",
    "        logger.warning(\"'ID' column not found in ais_test. Assigned index as 'ID'.\")\n",
    "\n",
    "    submission_df['ID'] = ais_test['ID'].values.astype(int)\n",
    "    submission_df = submission_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "    logger.debug(\"Reordered submission DataFrame columns.\")\n",
    "\n",
    "    logger.info(\"Submission DataFrame preparation completed.\")\n",
    "    return submission_df\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to run the machine learning pipeline.\"\"\"\n",
    "    logger.info(\"Starting the Machine Learning Pipeline.\")\n",
    "\n",
    "    # Print TensorFlow and Keras versions\n",
    "    logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "    logger.info(f\"Keras version: {tf.keras.__version__}\")\n",
    "\n",
    "    # List physical devices\n",
    "    physical_devices = tf.config.list_physical_devices()\n",
    "    logger.debug(f\"Physical devices: {physical_devices}\")\n",
    "\n",
    "    # Check for GPU availability\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        logger.info(\"TensorFlow is using the following GPU(s):\")\n",
    "        for gpu in gpus:\n",
    "            logger.info(f\"\\t{gpu}\")\n",
    "    else:\n",
    "        logger.warning(\"No GPU found. TensorFlow will use the CPU.\")\n",
    "\n",
    "    # Load datasets\n",
    "    ais_train, ais_test, vessels, ports, schedules = load_data()\n",
    "\n",
    "    # Verify column names\n",
    "    logger.debug(f\"AIS Train Columns: {list(ais_train.columns)}\")\n",
    "    logger.debug(f\"AIS Test Columns: {list(ais_test.columns)}\")\n",
    "    logger.debug(f\"Vessels Columns: {list(vessels.columns)}\")\n",
    "    logger.debug(f\"Ports Columns: {list(ports.columns)}\")\n",
    "\n",
    "    # Preprocess training data\n",
    "    merged_data, vessel_type_categories = preprocess_data(ais_train, vessels)\n",
    "\n",
    "    # Define features and target columns\n",
    "    features = ['hour', 'day_of_week', 'vessel_type_encoded']\n",
    "    target = ['future_latitude', 'future_longitude']\n",
    "    logger.debug(f\"Features: {features}\")\n",
    "    logger.debug(f\"Target: {target}\")\n",
    "\n",
    "    # Split data and prepare sequences\n",
    "    sequence_length = 5\n",
    "    X_train, y_train, X_val, y_val, val_data = split_data(merged_data, features, target, sequence_length)\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    input_shape = (sequence_length, len(features))\n",
    "    model = build_model(input_shape)\n",
    "    model.summary(print_fn=lambda x: logger.debug(x))\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mean_error_distance = evaluate_model(model, X_val, val_data, sequence_length)\n",
    "\n",
    "    # Prepare test data\n",
    "    merged_test, test_features = prepare_test_data(ais_test, vessels, vessel_type_categories)\n",
    "\n",
    "    # Prepare test sequences\n",
    "    X_test = prepare_test_sequences(merged_test, test_features, sequence_length)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    logger.info(\"Making predictions on the test set.\")\n",
    "    predictions_test = model.predict(X_test)\n",
    "    logger.debug(\"Predictions on test set obtained.\")\n",
    "\n",
    "    # Prepare submission DataFrame\n",
    "    submission_df = prepare_submission(ais_test, merged_test, predictions_test)\n",
    "\n",
    "    # Save submission file\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    logger.info(\"Submission file 'submission.csv' has been created.\")\n",
    "\n",
    "    logger.info(\"Machine Learning Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46300a0e-640e-44b7-905e-d26e5735b953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
