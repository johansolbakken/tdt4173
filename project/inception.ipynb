{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0000e4a5-4ad3-4dc8-9a51-02d465822eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "from typing import Tuple, Dict\n",
    "from colorlog import ColoredFormatter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8156b3c2-2141-4d48-bb8b-6e4db74e9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the colorful logger\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a colorful logger for the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"ML_Pipeline\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Define log colors for different levels\n",
    "    formatter = ColoredFormatter(\n",
    "        \"%(log_color)s%(levelname)-8s%(reset)s | %(log_color)s%(message)s%(reset)s\",\n",
    "        datefmt=None,\n",
    "        log_colors={\n",
    "            'DEBUG':    'cyan',\n",
    "            'INFO':     'white',\n",
    "            'WARNING':  'yellow',\n",
    "            'ERROR':    'red',\n",
    "            'CRITICAL': 'bold_red',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Stream handler for console output\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize the logger\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895abb9e-1bb5-4546-baa4-094df5cd2616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mLoading datasets.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mDatasets loaded successfully.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Filter and print rows where the heading is 511\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ais_train)):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mais_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheading\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m511\u001b[39m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load the AIS and auxiliary datasets.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the loaded DataFrames:\n",
    "        (ais_train, ais_test, vessels, ports, schedules).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading datasets.\")\n",
    "        \n",
    "        # Loading the AIS train and test data with date parsing\n",
    "        ais_train = pd.read_csv('ais_train.csv', sep='|', parse_dates=['time'], dtype={\n",
    "            'cog': 'float32', 'sog': 'float32', 'rot': 'int8', 'heading': 'int16', \n",
    "            'navstat': 'category', 'etaRaw': 'string', 'latitude': 'float64', \n",
    "            'longitude': 'float64', 'vesselId': 'string', 'portId': 'string'\n",
    "        })\n",
    "\n",
    "        ais_test = pd.read_csv('ais_test.csv', parse_dates=['time'], dtype={\n",
    "            'ID': 'string', 'vesselId': 'string', 'scaling_factor': 'float32'\n",
    "        })\n",
    "\n",
    "        # Loading vessels, ports, and schedules with specified separators and error handling\n",
    "        vessels = pd.read_csv('vessels.csv', sep='|', dtype={'vesselId': 'string', 'vesselType': 'string'})\n",
    "        ports = pd.read_csv('ports.csv', sep='|', dtype={'portId': 'string', 'portName': 'string', 'country': 'string'})\n",
    "        schedules = pd.read_csv('schedules_to_may_2024.csv', sep='|', on_bad_lines='skip', dtype={\n",
    "            'scheduleId': 'string', 'vesselId': 'string', 'portId': 'string', 'arrivalTime': 'string'\n",
    "        })\n",
    "\n",
    "        logger.info(\"Datasets loaded successfully.\")\n",
    "        \n",
    "        # Ensure the function returns the dataframes properly\n",
    "        return ais_train, ais_test, vessels, ports, schedules\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        logger.error(f\"Error parsing file: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the data\n",
    "ais_train, ais_test, vessels, ports, schedules = load_data()\n",
    "# Filter and print rows where the heading is 511\n",
    "for i in range(len(ais_train)):\n",
    "    if ais_train[i]['heading'] == 511:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1882031-64fd-40a7-bc7d-c00dcf01b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = ais_train.isnull().sum()\n",
    "print(\"Missing values per column in ais_train:\\n\", missing_values)\n",
    "\n",
    "# Handle outliers\n",
    "valid_latitude_range = (-90, 90)\n",
    "valid_longitude_range = (-180, 180)\n",
    "ais_train = ais_train[(ais_train['latitude'].between(*valid_latitude_range)) & \n",
    "                      (ais_train['longitude'].between(*valid_longitude_range))]\n",
    "\n",
    "max_sog = 50  # Maximum speed threshold\n",
    "ais_train = ais_train[ais_train['sog'] <= max_sog]\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "ais_train[['sog', 'cog']] = scaler.fit_transform(ais_train[['sog', 'cog']])\n",
    "\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22634fa2-f653-4069-b56f-5f128fb6053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n",
      "1521651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating Missing Days For Vessels:  11%|█▎           | 75/711 [00:16<01:48,  5.87vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  15%|█▊          | 104/711 [00:23<02:19,  4.35vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  20%|██▍         | 142/711 [00:29<01:23,  6.80vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  20%|██▍         | 143/711 [00:29<01:29,  6.37vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  27%|███▏        | 192/711 [00:43<01:44,  4.95vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  38%|████▌       | 273/711 [00:59<01:32,  4.71vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  43%|█████▏      | 304/711 [01:06<01:38,  4.13vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  43%|█████▏      | 308/711 [01:07<01:54,  3.51vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  44%|█████▏      | 310/711 [01:08<01:25,  4.67vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  45%|█████▍      | 320/711 [01:10<01:28,  4.44vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  46%|█████▍      | 324/711 [01:10<01:09,  5.58vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  47%|█████▋      | 337/711 [01:13<01:23,  4.50vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  63%|███████▌    | 450/711 [01:42<00:42,  6.18vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  70%|████████▍   | 498/711 [01:56<00:54,  3.90vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  71%|████████▍   | 503/711 [01:57<00:45,  4.58vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  71%|████████▌   | 508/711 [01:58<00:43,  4.65vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  76%|█████████   | 538/711 [02:04<00:33,  5.11vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  78%|█████████▎  | 552/711 [02:07<00:33,  4.75vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  82%|█████████▊  | 585/711 [02:15<01:18,  1.61vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  84%|██████████  | 596/711 [02:22<01:10,  1.63vessel/s]/var/folders/5q/k66vnm0d2ps_3_nhtc3cx0qr0000gn/T/ipykernel_4719/3821654240.py:61: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
      "Interpolating Missing Days For Vessels:  93%|███████████▏| 663/711 [02:43<00:11,  4.04vessel/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 192\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[1;32m    191\u001b[0m vessel_id_dict \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvesselId\u001b[39m\u001b[38;5;124m\"\u001b[39m]: i \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m vessels\u001b[38;5;241m.\u001b[39miterrows()}\n\u001b[0;32m--> 192\u001b[0m X_train, y_train, feature_scaler, target_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mais_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvessels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mports\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvessel_id_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 42\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(ais_train, vessels, ports, schedules, vessel_ids)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vessel_data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     41\u001b[0m     current_row \u001b[38;5;241m=\u001b[39m vessel_data\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m---> 42\u001b[0m     next_row \u001b[38;5;241m=\u001b[39m \u001b[43mvessel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     time_difference \u001b[38;5;241m=\u001b[39m next_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m current_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Add the current row to the interpolated data list\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/indexing.py:1754\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/frame.py:3996\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3994\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[1;32m   3995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3996\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3998\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3999\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/internals/managers.py:984\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    976\u001b[0m     block \u001b[38;5;241m=\u001b[39m new_block(\n\u001b[1;32m    977\u001b[0m         result,\n\u001b[1;32m    978\u001b[0m         placement\u001b[38;5;241m=\u001b[39mbp,\n\u001b[1;32m    979\u001b[0m         ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    980\u001b[0m         refs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrefs,\n\u001b[1;32m    981\u001b[0m     )\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SingleBlockManager(block, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 984\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[43minterleaved_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# TODO: use object dtype as workaround for non-performant\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m#  EA.__setitem__ methods. (primarily ArrowExtensionArray.__setitem__\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m#  when iteratively setting individual values)\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m#  https://github.com/pandas-dev/pandas/pull/54508#issuecomment-1675827918\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/internals/base.py:394\u001b[0m, in \u001b[0;36minterleaved_dtype\u001b[0;34m(dtypes)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_common_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:1465\u001b[0m, in \u001b[0;36mfind_common_type\u001b[0;34m(types)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, ExtensionDtype):\n\u001b[0;32m-> 1465\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_common_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1466\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1467\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/dtypes/dtypes.py:1595\u001b[0m, in \u001b[0;36mBaseMaskedDtype._get_common_dtype\u001b[0;34m(self, dtypes)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_common_dtype\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtypes: \u001b[38;5;28mlist\u001b[39m[DtypeObj]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DtypeObj \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# We unwrap any masked dtypes, find the common dtype we would use\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m#  for that, then re-mask the result.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_common_type\n\u001b[0;32m-> 1595\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mfind_common_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaseMaskedDtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;66;03m# If we ever support e.g. Masked[DatetimeArray] then this will change\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:1465\u001b[0m, in \u001b[0;36mfind_common_type\u001b[0;34m(types)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, ExtensionDtype):\n\u001b[0;32m-> 1465\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_common_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1466\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1467\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/dtypes/dtypes.py:681\u001b[0m, in \u001b[0;36mCategoricalDtype._get_common_dtype\u001b[0;34m(self, dtypes)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;66;03m# TODO should categorical always give an answer?\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_common_type\n\u001b[0;32m--> 681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_common_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_cat_dtypes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:1468\u001b[0m, in \u001b[0;36mfind_common_type\u001b[0;34m(types)\u001b[0m\n\u001b[1;32m   1466\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1467\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 1468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;66;03m# take lowest unit\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(lib\u001b[38;5;241m.\u001b[39mis_np_dtype(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_data(\n",
    "    ais_train: pd.DataFrame,\n",
    "    vessels: pd.DataFrame, \n",
    "    ports: pd.DataFrame, \n",
    "    schedules: pd.DataFrame,\n",
    "    vessel_ids: Dict\n",
    ") -> Tuple[np.ndarray, np.ndarray, MinMaxScaler, MinMaxScaler]:\n",
    "    \"\"\"Prepare the data for the LSTM model, including additional time-based, vessel-specific, and port proximity features.\n",
    "    \n",
    "    Args:\n",
    "        ais_train: DataFrame containing AIS training data.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing the feature array (X), target array (y), and the fitted scaler.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing data for the model.\")\n",
    "    \n",
    "    # Convert the 'time' column to datetime format for feature extraction\n",
    "    ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "    for col in ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'etaRaw']:\n",
    "        ais_train[col] = pd.to_numeric(ais_train[col], errors='coerce')\n",
    "\n",
    "    # Clip latitude and longitude to their valid ranges\n",
    "    ais_train['latitude'] = ais_train['latitude'].clip(-90, 90)\n",
    "    ais_train['longitude'] = ais_train['longitude'].clip(-180, 180)\n",
    "\n",
    "    print(len(ais_train))\n",
    "\n",
    "    # List to store the interpolated data points\n",
    "    interpolated_data = []\n",
    "\n",
    "    # Loop through each vessel ID\n",
    "    for vessel_id in tqdm(vessel_ids.keys(), desc=\"Interpolating Missing Days For Vessels\", unit=\"vessel\"):\n",
    "        # Filter and sort the data for the current vessel ID\n",
    "        vessel_data = ais_train[ais_train['vesselId'] == vessel_id].sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "        # Proceed only if vessel_data is not empty\n",
    "        if len(vessel_data) > 0:\n",
    "            # Loop through the sorted data to check time differences\n",
    "            for i in range(len(vessel_data) - 1):\n",
    "                current_row = vessel_data.iloc[i]\n",
    "                next_row = vessel_data.iloc[i + 1]\n",
    "\n",
    "                time_difference = next_row['time'] - current_row['time']\n",
    "                \n",
    "                # Add the current row to the interpolated data list\n",
    "                interpolated_data.append(current_row)\n",
    "\n",
    "                # Check if the time difference is greater than 1 day\n",
    "                if time_difference > pd.Timedelta(days=1):\n",
    "                    # Calculate the number of missing days\n",
    "                    num_missing_days = (time_difference.days - 1)\n",
    "\n",
    "                    # Linearly interpolate values for each missing day\n",
    "                    for day in range(1, num_missing_days + 1):\n",
    "                        interpolated_time = current_row['time'] + pd.Timedelta(days=day)\n",
    "                        \n",
    "                        # Interpolate all relevant columns\n",
    "                        interpolated_values = {}\n",
    "                        for col in ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'etaRaw']:\n",
    "                            value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
    "                            interpolated_values[col] = current_row[col] + value_diff * day\n",
    "                        \n",
    "                        # Create a new interpolated data point\n",
    "                        interpolated_point = current_row.copy()\n",
    "                        interpolated_point['time'] = interpolated_time\n",
    "                        interpolated_point['latitude'] = interpolated_values['latitude']\n",
    "                        interpolated_point['longitude'] = interpolated_values['longitude']\n",
    "                        # interpolated_point['cog'] = interpolated_values['cog']\n",
    "                        # interpolated_point['sog'] = interpolated_values['sog']\n",
    "                        # interpolated_point['rot'] = interpolated_values['rot']\n",
    "                        # interpolated_point['heading'] = interpolated_values['heading']\n",
    "                        # interpolated_point['etaRaw'] = interpolated_values['etaRaw']\n",
    "\n",
    "                        # Copy the values of 'vesselId', 'portId', and 'navstat' directly from the current row\n",
    "                        interpolated_point['vesselId'] = current_row['vesselId']\n",
    "                        # interpolated_point['portId'] = current_row['portId']\n",
    "                        # interpolated_point['navstat'] = current_row['navstat']\n",
    "\n",
    "                        # Add the interpolated point to the list\n",
    "                        interpolated_data.append(interpolated_point)\n",
    "\n",
    "            # Add the last row to the interpolated data list\n",
    "            interpolated_data.append(vessel_data.iloc[-1])\n",
    "    # Convert the list of interpolated data back into a DataFrame\n",
    "    interpolated_df = pd.DataFrame(interpolated_data)\n",
    "\n",
    "    # Combine the interpolated data with the original ais_train DataFrame\n",
    "    combined_df = pd.concat([ais_train, interpolated_df]).drop_duplicates().sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    ais_train = combined_df\n",
    "\n",
    "    print(len(ais_train))\n",
    "\n",
    "    # Extract hour of the day and day of the week as new features\n",
    "    ais_train['hour_of_day'] = ais_train['time'].dt.hour\n",
    "    ais_train['day_of_week'] = ais_train['time'].dt.dayofweek\n",
    "\n",
    "    # Calculate the time elapsed since the first recorded entry for each vessel\n",
    "    ais_train['time_elapsed'] = (ais_train['time'] - ais_train['time'].min()).dt.total_seconds()\n",
    "   \n",
    "    # Map vesselId to its encoded value using vessel_ids dictionary\n",
    "    ais_train['vesselId_encoded'] = ais_train['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "\n",
    "    # Compute the sine and cosine of the course over ground (cog) to represent direction\n",
    "    ais_train['cog_sin'] = np.sin(np.deg2rad(ais_train['cog']))\n",
    "    ais_train['cog_cos'] = np.cos(np.deg2rad(ais_train['cog']))\n",
    "\n",
    "    # Categorize the speed of the vessel\n",
    "    ais_train['speed_category'] = pd.cut(ais_train['sog'], bins=[-1, 5, 15, np.inf], labels=[0, 1, 2])\n",
    "\n",
    "    # Merge vessel-specific information into ais_train data\n",
    "    ais_train = ais_train.merge(vessels[['vesselId', 'maxSpeed', 'length', 'yearBuilt']], on='vesselId', how='left')\n",
    "\n",
    "    # Fill missing values in vessel-specific data with appropriate defaults\n",
    "    ais_train['maxSpeed'] = ais_train['maxSpeed'].fillna(ais_train['maxSpeed'].mean())\n",
    "\n",
    "    # Measure time to create the R-tree index for ports\n",
    "    start_time = time.time()\n",
    "    port_idx = index.Index()\n",
    "    for idx, row in ports.iterrows():\n",
    "        port_idx.insert(idx, (row['longitude'], row['latitude'], row['longitude'], row['latitude']))\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Time to build R-tree index for ports: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Measure time to calculate the nearest port for each vessel with progress and ETA\n",
    "    total_vessels = len(ais_train)\n",
    "    start_time = time.time()\n",
    "    closest_ports = []\n",
    "\n",
    "    for _, vessel in tqdm(ais_train.iterrows(), total=total_vessels, desc=\"Calculating nearest ports\", unit=\"vessel\"):\n",
    "        point = Point(vessel['longitude'], vessel['latitude'])\n",
    "        nearest_port_idx = list(port_idx.nearest((vessel['longitude'], vessel['latitude'], vessel['longitude'], vessel['latitude']), 1))[0]\n",
    "        closest_port = ports.iloc[nearest_port_idx]\n",
    "        distance_to_port = point.distance(Point(closest_port['longitude'], closest_port['latitude']))\n",
    "        closest_ports.append(distance_to_port)\n",
    "\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Time to calculate closest port for all vessels: {end_time - start_time:.2f} seconds for {total_vessels} vessels\")\n",
    "\n",
    "    # Add the calculated distances to the AIS data\n",
    "    ais_train['distance_to_nearest_port'] = closest_ports\n",
    "\n",
    "    speed_threshold = 5.0  # knots\n",
    "    distance_threshold = 1.0  # kilometers\n",
    "\n",
    "    # Identify if the vessel is anchored\n",
    "    ais_train['anchored'] = (ais_train['sog'] < speed_threshold) & (ais_train['distance_to_nearest_port'] < distance_threshold)\n",
    "\n",
    "\n",
    "    # Extract the relevant features, including the new ones\n",
    "    features = ais_train[['latitude', 'longitude', 'sog', 'day_of_week', 'distance_to_nearest_port', 'anchored', 'vesselId_encoded', 'time_elapsed']].values\n",
    "    target = ais_train[['latitude', 'longitude']].shift(-1).ffill().values\n",
    "\n",
    "    # Normalize features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "   \n",
    "    # Prepare the features and target values for each vessel separately\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    # Iterate over each vessel ID in the dataset\n",
    "    for vessel_id in tqdm(vessel_ids.keys(), total=len(vessel_ids), desc=\"Finding y 5 days forward\", unit=\"vessel\"):\n",
    "        # Filter data for the current vessel and sort it by time\n",
    "        vessel_data = ais_train[ais_train['vesselId'] == vessel_id].reset_index(drop=True)\n",
    "\n",
    "        # Ensure that we have at least 5 days of data for this vessel\n",
    "        for i in range(len(vessel_data) - 5):\n",
    "            # Append the feature set (X) for the current day\n",
    "            X_list.append(vessel_data.iloc[i].values)\n",
    "\n",
    "            # Append the target set (y) which is the position 5 days later\n",
    "            y_list.append(vessel_data.iloc[i + 5][['latitude', 'longitude']].values)\n",
    "\n",
    "    # Convert the collected lists to numpy arrays\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # Reshape for LSTM input: (samples, timesteps, features)\n",
    "    X = features_scaled.reshape((features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
    "    \n",
    "    # Normalize target data (latitude and longitude)\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y = target_scaler.fit_transform(target)    \n",
    "    \n",
    "    logger.info(\"Data preparation complete.\")\n",
    "    return X, y, feature_scaler, target_scaler\n",
    "\n",
    "# Prepare the data\n",
    "vessel_id_dict = {row[\"vesselId\"]: i for i, row in vessels.iterrows()}\n",
    "X_train, y_train, feature_scaler, target_scaler = prepare_data(ais_train, vessels, ports, schedules, vessel_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed2a8d6-c490-498a-bb43-6af56b5fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def geodesic_loss(y_true, y_pred):\n",
    "    \"\"\"Calculate the Haversine distance between true and predicted coordinates.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor of true coordinates (latitude, longitude).\n",
    "        y_pred: Tensor of predicted coordinates (latitude, longitude).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor representing the geodesic distance (Haversine distance) between the true and predicted points.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Split the latitude and longitude into separate tensors\n",
    "    lat_true, lon_true = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
    "    lat_pred, lon_pred = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "    \n",
    "    # Convert degrees to radians manually\n",
    "    lat_true = lat_true * tf.constant(np.pi / 180.0)\n",
    "    lon_true = lon_true * tf.constant(np.pi / 180.0)\n",
    "    lat_pred = lat_pred * tf.constant(np.pi / 180.0)\n",
    "    lon_pred = lon_pred * tf.constant(np.pi / 180.0)\n",
    "    \n",
    "    # Compute the differences between true and predicted coordinates\n",
    "    dlat = lat_pred - lat_true\n",
    "    dlon = lon_pred - lon_true\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = tf.square(tf.sin(dlat / 2)) + tf.cos(lat_true) * tf.cos(lat_pred) * tf.square(tf.sin(dlon / 2))\n",
    "    c = 2 * tf.atan2(tf.sqrt(a), tf.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return tf.reduce_mean(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f38c7f-8f1d-489a-b718-7b0547cd1c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape: Tuple[int, int]) -> Sequential:\n",
    "    \"\"\"Build the LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of the input data (timesteps, features).\n",
    "        \n",
    "    Returns:\n",
    "        Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Building the LSTM model.\")\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))  # Use Input layer to specify the shape\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=2))  # Output: latitude and longitude\n",
    "    model.compile(optimizer='adam', loss=geodesic_loss)\n",
    "    logger.info(\"Model built successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=(X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "892aa819-c46d-465a-8df1-0cb4101faf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "Epoch 1/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 934us/step - loss: 24.9886 - val_loss: 35.4740 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 975us/step - loss: 23.6152 - val_loss: 19.8003 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1ms/step - loss: 23.5554 - val_loss: 25.7895 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1ms/step - loss: 23.5387 - val_loss: 27.1325 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1ms/step - loss: 23.4886 - val_loss: 27.8399 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1ms/step - loss: 23.4468 - val_loss: 26.8645 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1ms/step - loss: 23.4379 - val_loss: 27.5344 - learning_rate: 5.0000e-04\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def train_model(model: Sequential, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "    \"\"\"Train the LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: The LSTM model to train.\n",
    "        X_train: Training features.\n",
    "        y_train: Training targets.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting model training.\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2,callbacks=[early_stopping, reduce_lr])\n",
    "    logger.info(\"Model training complete.\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c9326df-2db0-4995-92bd-0d5a933b3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[1m1617/1617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438us/step\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generate_submission(model: Sequential, ais_test: pd.DataFrame, feature_scaler: MinMaxScaler, target_scaler: MinMaxScaler, vessel_ids: Dict) -> None:\n",
    "    \"\"\"Generate a submission file with the predicted vessel positions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        ais_test: DataFrame containing AIS test data.\n",
    "        feature_scaler: Scaler used to normalize the features.\n",
    "        target_scaler: Scaler used to normalize the target coordinates.\n",
    "        vessel_ids: Dictionary to map vessel IDs to numerical indices.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating predictions for the test set.\")\n",
    "    \n",
    "    # Convert the 'time' column to datetime format to handle arithmetic operations\n",
    "    ais_test['time'] = pd.to_datetime(ais_test['time'], errors='coerce')\n",
    "    \n",
    "    # Map vesselId to its encoded value using vessel_ids dictionary\n",
    "    ais_test['vesselId_encoded'] = ais_test['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "   \n",
    "    # Calculate the time elapsed since the first recorded entry for each vessel\n",
    "    ais_test['time_elapsed'] = (ais_test['time'] - ais_test['time'].min()).dt.total_seconds()\n",
    "\n",
    "    # Extract the relevant features for the test data\n",
    "    test_features = ais_test[['vesselId_encoded', 'time_elapsed']].values\n",
    "\n",
    "    # Since the test data only has one feature, we need to adjust the input shape to match the model's expectation\n",
    "    num_train_features = feature_scaler.n_features_in_\n",
    "    test_features_padded = np.zeros((test_features.shape[0], num_train_features))\n",
    "    test_features_padded[:, :test_features.shape[1]] = test_features\n",
    "\n",
    "    # Normalize the padded test features to match the training data scale\n",
    "    test_features_scaled = feature_scaler.transform(test_features_padded)\n",
    "    X_test = test_features_scaled.reshape((test_features_scaled.shape[0], 1, test_features_scaled.shape[1]))\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform the predictions using the target scaler\n",
    "    predictions = target_scaler.inverse_transform(predictions)\n",
    "    \n",
    "    # Create the submission DataFrame in the required format\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': ais_test['ID'],\n",
    "        'longitude_predicted': predictions[:, 1],\n",
    "        'latitude_predicted': predictions[:, 0]\n",
    "    })\n",
    "   \n",
    "    # Ensure that the submission file has exactly 51739 rows as required\n",
    "    assert submission.shape[0] == 51739, \"The submission file must have exactly 51739 rows.\"\n",
    "    \n",
    "    # Save the predictions to submission.csv\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    logger.info(\"Submission file saved as submission.csv.\")\n",
    "\n",
    "\n",
    "# Generate the submission file\n",
    "generate_submission(model, ais_test, feature_scaler, target_scaler, vessel_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccf0fcd-cdbb-44f4-8acc-5d62a2845a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Longitude Predicted: 28.37741\n",
      "Minimum Longitude Predicted: 3.8418357\n",
      "Maximum Latitude Predicted: 56.508556\n",
      "Minimum Latitude Predicted: 38.977943\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('submission.csv')\n",
    "\n",
    "# Calculate the maximum and minimum values for longitude_predicted and latitude_predicted\n",
    "longitude_max = df['longitude_predicted'].max()\n",
    "longitude_min = df['longitude_predicted'].min()\n",
    "latitude_max = df['latitude_predicted'].max()\n",
    "latitude_min = df['latitude_predicted'].min()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum Longitude Predicted: {longitude_max}\")\n",
    "print(f\"Minimum Longitude Predicted: {longitude_min}\")\n",
    "print(f\"Maximum Latitude Predicted: {latitude_max}\")\n",
    "print(f\"Minimum Latitude Predicted: {latitude_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e26519-4a82-4e70-bb5a-c8a6d30fc710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating nearest ports: 100%|██████████████████████████████████████████████████████████████████████████████| 1522065/1522065 [02:12<00:00, 11528.05vessel/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 553us/step - loss: 0.0093\n",
      "Epoch 2/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 552us/step - loss: 4.3165e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 552us/step - loss: 2.9144e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 555us/step - loss: 2.2083e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 550us/step - loss: 1.8692e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 558us/step - loss: 1.6073e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 560us/step - loss: 1.4307e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 561us/step - loss: 1.2737e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 582us/step - loss: 1.1609e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 558us/step - loss: 1.1753e-05\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 305us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 549us/step - loss: 2623.6301\n",
      "Epoch 2/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m719s\u001b[0m 15ms/step - loss: 2595.5369\n",
      "Epoch 3/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 552us/step - loss: 2587.8733\n",
      "Epoch 4/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 8ms/step - loss: 2576.3860\n",
      "Epoch 5/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 562us/step - loss: 2562.2231\n",
      "Epoch 6/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 551us/step - loss: 2534.6355\n",
      "Epoch 7/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 555us/step - loss: 2525.0088\n",
      "Epoch 8/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 569us/step - loss: 2505.4629\n",
      "Epoch 9/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 578us/step - loss: 2471.6233\n",
      "Epoch 10/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 558us/step - loss: 2445.7361\n",
      "\u001b[1m1617/1617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 329us/step\n",
      "Submission file created successfully with 51739 rows.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from rtree import index\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Load the data\n",
    "ais_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "ais_test = pd.read_csv('ais_test.csv')\n",
    "ports = pd.read_csv('ports.csv', sep='|')  # Assuming you have a ports.csv file to load port data\n",
    "\n",
    "# Measure time to create the R-tree index for ports\n",
    "start_time = time.time()\n",
    "port_idx = index.Index()\n",
    "for idx, row in ports.iterrows():\n",
    "    port_idx.insert(idx, (row['longitude'], row['latitude'], row['longitude'], row['latitude']))\n",
    "end_time = time.time()\n",
    "logger.info(f\"Time to build R-tree index for ports: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure time to calculate the nearest port for each vessel with progress and ETA\n",
    "total_vessels = len(ais_train)\n",
    "start_time = time.time()\n",
    "closest_ports = []\n",
    "\n",
    "for _, vessel in tqdm(ais_train.iterrows(), total=total_vessels, desc=\"Calculating nearest ports\", unit=\"vessel\"):\n",
    "    point = Point(vessel['longitude'], vessel['latitude'])\n",
    "    nearest_port_idx = list(port_idx.nearest((vessel['longitude'], vessel['latitude'], vessel['longitude'], vessel['latitude']), 1))[0]\n",
    "    closest_port = ports.iloc[nearest_port_idx]\n",
    "    distance_to_port = point.distance(Point(closest_port['longitude'], closest_port['latitude']))\n",
    "    closest_ports.append(distance_to_port)\n",
    "\n",
    "end_time = time.time()\n",
    "logger.info(f\"Time to calculate closest port for all vessels: {end_time - start_time:.2f} seconds for {total_vessels} vessels\")\n",
    "\n",
    "# Add the calculated distances to the AIS data\n",
    "ais_train['distance_to_nearest_port'] = closest_ports\n",
    "\n",
    "speed_threshold = 5.0  # knots\n",
    "distance_threshold = 1.0  # kilometers\n",
    "\n",
    "# Identify if the vessel is anchored\n",
    "ais_train['anchored'] = (ais_train['sog'] < speed_threshold) & (ais_train['distance_to_nearest_port'] < distance_threshold)\n",
    "\n",
    "# Convert 'time' column to datetime and extract useful features like hour, minute, and total time elapsed\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'], errors='coerce')\n",
    "ais_train['hour'] = ais_train['time'].dt.hour\n",
    "ais_train['minute'] = ais_train['time'].dt.minute\n",
    "\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'], errors='coerce')\n",
    "ais_test['hour'] = ais_test['time'].dt.hour\n",
    "ais_test['minute'] = ais_test['time'].dt.minute\n",
    "\n",
    "# Calculate the total time elapsed in seconds from the earliest time in the dataset\n",
    "reference_time = ais_train['time'].min()\n",
    "ais_train['time_elapsed'] = (ais_train['time'] - reference_time).dt.total_seconds()\n",
    "ais_test['time_elapsed'] = (ais_test['time'] - reference_time).dt.total_seconds()\n",
    "\n",
    "# Fill missing values in 'hour', 'minute', and 'time_elapsed' with a placeholder (e.g., -1) to handle NaN values\n",
    "ais_train[['hour', 'minute', 'time_elapsed']] = ais_train[['hour', 'minute', 'time_elapsed']].fillna(-1)\n",
    "ais_test[['hour', 'minute', 'time_elapsed']] = ais_test[['hour', 'minute', 'time_elapsed']].fillna(-1)\n",
    "\n",
    "# Encode categorical features like 'vesselId' using Label Encoding\n",
    "le = LabelEncoder()\n",
    "ais_train['vesselId_encoded'] = le.fit_transform(ais_train['vesselId'].astype(str))\n",
    "ais_test['vesselId_encoded'] = le.transform(ais_test['vesselId'].astype(str))\n",
    "\n",
    "# Define numeric columns for training separately, including the new time_elapsed feature\n",
    "train_numeric_columns = ['cog', 'sog', 'rot', 'heading', 'navstat', 'etaRaw', 'latitude', 'longitude', 'vesselId_encoded', 'time_elapsed', 'distance_to_nearest_port', 'anchored']\n",
    "\n",
    "# Convert training columns to numeric, coercing errors to NaN\n",
    "ais_train[train_numeric_columns] = ais_train[train_numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill any remaining NaN values in the dataset with appropriate defaults\n",
    "ais_train[train_numeric_columns] = ais_train[train_numeric_columns].fillna(0)\n",
    "\n",
    "# Extract features and targets for the first model using the full training set\n",
    "X_train_full = ais_train[train_numeric_columns].values\n",
    "y_train_full = ais_train[['latitude', 'longitude']].values\n",
    "\n",
    "# Normalize/Scale the full training set\n",
    "scaler_X_train_full = StandardScaler()\n",
    "X_train_full = scaler_X_train_full.fit_transform(X_train_full)\n",
    "\n",
    "scaler_y_full = StandardScaler()\n",
    "y_train_full = scaler_y_full.fit_transform(y_train_full)\n",
    "\n",
    "# Reshape the data for LSTM input (samples, timesteps, features)\n",
    "X_train_full = X_train_full.reshape(X_train_full.shape[0], 1, X_train_full.shape[1])\n",
    "\n",
    "# Build and train the first model on the full training set\n",
    "input_full = Input(shape=(1, X_train_full.shape[2]), name='full_input')\n",
    "lstm_full = LSTM(64, return_sequences=False)(input_full)\n",
    "dense_full = Dense(32, activation='relu')(lstm_full)\n",
    "output_full = Dense(2, activation='linear', name='coordinates_output')(dense_full)\n",
    "\n",
    "model_full = Model(inputs=input_full, outputs=output_full)\n",
    "model_full.compile(optimizer='adam', loss='mse')\n",
    "model_full.fit(X_train_full, y_train_full, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the full training set using the first model\n",
    "predicted_coords = model_full.predict(X_train_full)\n",
    "predicted_coords = scaler_y_full.inverse_transform(predicted_coords)  # Reverse scaling\n",
    "\n",
    "# Prepare input data for the second model using vesselId, hour, minute, and time_elapsed features\n",
    "X_train_second_model = ais_train[['vesselId_encoded', 'time_elapsed']].values\n",
    "y_train_second_model = predicted_coords  # Use the predicted coordinates as labels\n",
    "\n",
    "# Normalize/Scale the input data for the second model\n",
    "scaler_X_second = StandardScaler()\n",
    "X_train_second_model = scaler_X_second.fit_transform(X_train_second_model)\n",
    "\n",
    "# Reshape the data for LSTM input (samples, timesteps, features)\n",
    "X_train_second_model = X_train_second_model.reshape(X_train_second_model.shape[0], 1, X_train_second_model.shape[1])\n",
    "\n",
    "# Build and train the second model on vesselId, time, and time_elapsed\n",
    "input_second = Input(shape=(1, X_train_second_model.shape[2]), name='second_input')\n",
    "lstm_second = LSTM(64, return_sequences=False)(input_second)\n",
    "dense_second = Dense(32, activation='relu')(lstm_second)\n",
    "output_second = Dense(2, activation='linear', name='coordinates_predicted')(dense_second)\n",
    "\n",
    "model_second = Model(inputs=input_second, outputs=output_second)\n",
    "model_second.compile(optimizer='adam', loss='mse')\n",
    "model_second.fit(X_train_second_model, y_train_second_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the test set using the second model\n",
    "X_test_subset = ais_test[['vesselId_encoded', 'time_elapsed']].values\n",
    "X_test_subset = scaler_X_second.transform(X_test_subset)\n",
    "X_test_subset = X_test_subset.reshape(X_test_subset.shape[0], 1, X_test_subset.shape[1])\n",
    "\n",
    "test_predictions = model_second.predict(X_test_subset)\n",
    "\n",
    "# Reverse the scaling to get the original longitude and latitude values\n",
    "test_predictions = scaler_y_full.inverse_transform(test_predictions)\n",
    "\n",
    "# Generate the submission file with the predicted vessel positions\n",
    "submission = pd.DataFrame({\n",
    "    'ID': ais_test['ID'],\n",
    "    'longitude_predicted': test_predictions[:, 1],\n",
    "    'latitude_predicted': test_predictions[:, 0]\n",
    "})\n",
    "\n",
    "# Ensure that the submission file has exactly 51739 rows as required\n",
    "assert submission.shape[0] == 51739, \"The submission file must have exactly 51739 rows.\"\n",
    "\n",
    "# Save the submission to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully with 51739 rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
