{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0000e4a5-4ad3-4dc8-9a51-02d465822eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "from typing import Tuple, Dict\n",
    "from colorlog import ColoredFormatter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8156b3c2-2141-4d48-bb8b-6e4db74e9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the colorful logger\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a colorful logger for the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"ML_Pipeline\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Define log colors for different levels\n",
    "    formatter = ColoredFormatter(\n",
    "        \"%(log_color)s%(levelname)-8s%(reset)s | %(log_color)s%(message)s%(reset)s\",\n",
    "        datefmt=None,\n",
    "        log_colors={\n",
    "            'DEBUG':    'cyan',\n",
    "            'INFO':     'white',\n",
    "            'WARNING':  'yellow',\n",
    "            'ERROR':    'red',\n",
    "            'CRITICAL': 'bold_red',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Stream handler for console output\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize the logger\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895abb9e-1bb5-4546-baa4-094df5cd2616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mLoading datasets.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mDatasets loaded successfully.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cog</th>\n",
       "      <th>sog</th>\n",
       "      <th>rot</th>\n",
       "      <th>heading</th>\n",
       "      <th>navstat</th>\n",
       "      <th>etaRaw</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>portId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:25</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>01-09 23:00</td>\n",
       "      <td>-34.74370</td>\n",
       "      <td>-57.85130</td>\n",
       "      <td>61e9f3a8b937134a3c4bfdf7</td>\n",
       "      <td>61d371c43aeaecc07011a37f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:00:36</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>12-29 20:00</td>\n",
       "      <td>8.89440</td>\n",
       "      <td>-79.47939</td>\n",
       "      <td>61e9f3d4b937134a3c4bff1f</td>\n",
       "      <td>634c4de270937fc01c3a7689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:01:45</td>\n",
       "      <td>111.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>01-02 09:00</td>\n",
       "      <td>39.19065</td>\n",
       "      <td>-76.47567</td>\n",
       "      <td>61e9f436b937134a3c4c0131</td>\n",
       "      <td>61d3847bb7b7526e1adf3d19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:03:11</td>\n",
       "      <td>96.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>12-31 20:00</td>\n",
       "      <td>-34.41189</td>\n",
       "      <td>151.02067</td>\n",
       "      <td>61e9f3b4b937134a3c4bfe77</td>\n",
       "      <td>61d36f770a1807568ff9a126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:03:51</td>\n",
       "      <td>214.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>01-25 12:00</td>\n",
       "      <td>35.88379</td>\n",
       "      <td>-5.91636</td>\n",
       "      <td>61e9f41bb937134a3c4c0087</td>\n",
       "      <td>634c4de270937fc01c3a74f3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    cog   sog  rot  heading  navstat       etaRaw  \\\n",
       "0  2024-01-01 00:00:25  284.0   0.7    0       88        0  01-09 23:00   \n",
       "1  2024-01-01 00:00:36  109.6   0.0   -6      347        1  12-29 20:00   \n",
       "2  2024-01-01 00:01:45  111.0  11.0    0      112        0  01-02 09:00   \n",
       "3  2024-01-01 00:03:11   96.4   0.0    0      142        1  12-31 20:00   \n",
       "4  2024-01-01 00:03:51  214.0  19.7    0      215        0  01-25 12:00   \n",
       "\n",
       "   latitude  longitude                  vesselId                    portId  \n",
       "0 -34.74370  -57.85130  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  \n",
       "1   8.89440  -79.47939  61e9f3d4b937134a3c4bff1f  634c4de270937fc01c3a7689  \n",
       "2  39.19065  -76.47567  61e9f436b937134a3c4c0131  61d3847bb7b7526e1adf3d19  \n",
       "3 -34.41189  151.02067  61e9f3b4b937134a3c4bfe77  61d36f770a1807568ff9a126  \n",
       "4  35.88379   -5.91636  61e9f41bb937134a3c4c0087  634c4de270937fc01c3a74f3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load the AIS and auxiliary datasets.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the loaded DataFrames:\n",
    "        (ais_train, ais_test, vessels, ports, schedules).\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading datasets.\")\n",
    "    ais_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "    ais_test = pd.read_csv('ais_test.csv')\n",
    "    vessels = pd.read_csv('vessels.csv', sep='|')\n",
    "    ports = pd.read_csv('ports.csv', sep='|')\n",
    "    schedules = pd.read_csv('schedules_to_may_2024.csv', sep='|', on_bad_lines='skip')\n",
    "    logger.info(\"Datasets loaded successfully.\")\n",
    "    return ais_train, ais_test, vessels, ports, schedules\n",
    "\n",
    "# Load the data\n",
    "ais_train, ais_test, vessels, ports, schedules = load_data()\n",
    "ais_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22634fa2-f653-4069-b56f-5f128fb6053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating Missing Days For Vessels: 100%|██████████████████████████| 711/711 [02:05<00:00,  5.67vessel/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mData preparation complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mData preparation complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mData preparation complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def prepare_data(\n",
    "    ais_train: pd.DataFrame,\n",
    "    vessels: pd.DataFrame, \n",
    "    ports: pd.DataFrame, \n",
    "    schedules: pd.DataFrame,\n",
    "    vessel_ids: Dict\n",
    ") -> Tuple[np.ndarray, np.ndarray, MinMaxScaler, MinMaxScaler]:\n",
    "    \"\"\"Prepare the data for the LSTM model, including additional time-based, vessel-specific, and port proximity features.\n",
    "    \n",
    "    Args:\n",
    "        ais_train: DataFrame containing AIS training data.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing the feature array (X), target array (y), and the fitted scaler.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing data for the model.\")\n",
    "    \n",
    "    # Convert the 'time' column to datetime format for feature extraction\n",
    "    ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "    for col in ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'etaRaw']:\n",
    "        ais_train[col] = pd.to_numeric(ais_train[col], errors='coerce')\n",
    "\n",
    "    # Clip latitude and longitude to their valid ranges\n",
    "    ais_train['latitude'] = ais_train['latitude'].clip(-90, 90)\n",
    "    ais_train['longitude'] = ais_train['longitude'].clip(-180, 180)\n",
    "\n",
    "    # List to store the interpolated data points\n",
    "    interpolated_data = []\n",
    "\n",
    "    # Loop through each vessel ID\n",
    "    for vessel_id in tqdm(vessel_ids.keys(), desc=\"Interpolating Missing Days For Vessels\", unit=\"vessel\"):\n",
    "        # Filter and sort the data for the current vessel ID\n",
    "        vessel_data = ais_train[ais_train['vesselId'] == vessel_id].sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "        # Proceed only if vessel_data is not empty\n",
    "        if len(vessel_data) > 0:\n",
    "            # Loop through the sorted data to check time differences\n",
    "            for i in range(len(vessel_data) - 1):\n",
    "                current_row = vessel_data.iloc[i]\n",
    "                next_row = vessel_data.iloc[i + 1]\n",
    "\n",
    "                time_difference = next_row['time'] - current_row['time']\n",
    "                \n",
    "                # Add the current row to the interpolated data list\n",
    "                interpolated_data.append(current_row)\n",
    "\n",
    "                # Check if the time difference is greater than 1 day\n",
    "                if time_difference > pd.Timedelta(days=1):\n",
    "                    # Calculate the number of missing days\n",
    "                    num_missing_days = (time_difference.days - 1)\n",
    "\n",
    "                    # Linearly interpolate values for each missing day\n",
    "                    for day in range(1, num_missing_days + 1):\n",
    "                        interpolated_time = current_row['time'] + pd.Timedelta(days=day)\n",
    "                        \n",
    "                        # Interpolate all relevant columns\n",
    "                        interpolated_values = {}\n",
    "                        for col in ['latitude', 'longitude']:\n",
    "                            value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
    "                            interpolated_values[col] = current_row[col] + value_diff * day\n",
    "                        \n",
    "                        # Create a new interpolated data point\n",
    "                        interpolated_point = current_row.copy()\n",
    "                        interpolated_point['time'] = interpolated_time\n",
    "                        interpolated_point['latitude'] = interpolated_values['latitude']\n",
    "                        interpolated_point['longitude'] = interpolated_values['longitude']\n",
    "                        interpolated_point['vesselId'] = current_row['vesselId']\n",
    "\n",
    "                        # Add the interpolated point to the list\n",
    "                        interpolated_data.append(interpolated_point)\n",
    "\n",
    "            # Add the last row to the interpolated data list\n",
    "            interpolated_data.append(vessel_data.iloc[-1])\n",
    "    \n",
    "    # Convert the list of interpolated data back into a DataFrame\n",
    "    interpolated_df = pd.DataFrame(interpolated_data)\n",
    "\n",
    "    # Combine the interpolated data with the original ais_train DataFrame\n",
    "    combined_df = pd.concat([ais_train, interpolated_df]).drop_duplicates().sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    ais_train = combined_df\n",
    "\n",
    "    # Calculate the time elapsed since the first recorded entry for each vessel\n",
    "    ais_train['time_elapsed'] = (ais_train['time'] - ais_train['time'].min()).dt.total_seconds()\n",
    "   \n",
    "    # Map vesselId to its encoded value using vessel_ids dictionary\n",
    "    ais_train['vesselId_encoded'] = ais_train['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "\n",
    "    # Extract the relevant features, including the new ones\n",
    "    features = ais_train[['vesselId_encoded', 'time_elapsed']].values\n",
    "    target = ais_train[['latitude', 'longitude']].shift(-1).ffill().values\n",
    "\n",
    "    # Normalize features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "    \n",
    "    y = ais_train[['latitude', 'longitude']].shift(-1).ffill().values\n",
    "\n",
    "    # Reshape for LSTM input: (samples, timesteps, features)\n",
    "    X = features_scaled.reshape((features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
    "    \n",
    "    # Normalize target data (latitude and longitude)\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y = target_scaler.fit_transform(target)    \n",
    "    \n",
    "    logger.info(\"Data preparation complete.\")\n",
    "    return X, y, feature_scaler, target_scaler\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "vessel_id_dict = {row[\"vesselId\"]: i for i, row in vessels.iterrows()}\n",
    "X_train, y_train, feature_scaler, target_scaler = prepare_data(ais_train, vessels, ports, schedules, vessel_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed2a8d6-c490-498a-bb43-6af56b5fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def geodesic_loss(y_true, y_pred):\n",
    "    \"\"\"Calculate the Haversine distance between true and predicted coordinates.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor of true coordinates (latitude, longitude).\n",
    "        y_pred: Tensor of predicted coordinates (latitude, longitude).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor representing the geodesic distance (Haversine distance) between the true and predicted points.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Split the latitude and longitude into separate tensors\n",
    "    lat_true, lon_true = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
    "    lat_pred, lon_pred = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "    \n",
    "    # Convert degrees to radians manually\n",
    "    lat_true = lat_true * tf.constant(np.pi / 180.0)\n",
    "    lon_true = lon_true * tf.constant(np.pi / 180.0)\n",
    "    lat_pred = lat_pred * tf.constant(np.pi / 180.0)\n",
    "    lon_pred = lon_pred * tf.constant(np.pi / 180.0)\n",
    "    \n",
    "    # Compute the differences between true and predicted coordinates\n",
    "    dlat = lat_pred - lat_true\n",
    "    dlon = lon_pred - lon_true\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = tf.square(tf.sin(dlat / 2)) + tf.cos(lat_true) * tf.cos(lat_pred) * tf.square(tf.sin(dlon / 2))\n",
    "    c = 2 * tf.atan2(tf.sqrt(a), tf.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return tf.reduce_mean(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f38c7f-8f1d-489a-b718-7b0547cd1c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape: Tuple[int, int]) -> Sequential:\n",
    "    \"\"\"Build the LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of the input data (timesteps, features).\n",
    "        \n",
    "    Returns:\n",
    "        Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Building the LSTM model.\")\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))  # Use Input layer to specify the shape\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=2))  # Output: latitude and longitude\n",
    "    model.compile(optimizer='adam', loss=geodesic_loss)\n",
    "    logger.info(\"Model built successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=(X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "892aa819-c46d-465a-8df1-0cb4101faf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "Epoch 1/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 934us/step - loss: 24.9886 - val_loss: 35.4740 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 975us/step - loss: 23.6152 - val_loss: 19.8003 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1ms/step - loss: 23.5554 - val_loss: 25.7895 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1ms/step - loss: 23.5387 - val_loss: 27.1325 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1ms/step - loss: 23.4886 - val_loss: 27.8399 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1ms/step - loss: 23.4468 - val_loss: 26.8645 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1ms/step - loss: 23.4379 - val_loss: 27.5344 - learning_rate: 5.0000e-04\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def train_model(model: Sequential, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "    \"\"\"Train the LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: The LSTM model to train.\n",
    "        X_train: Training features.\n",
    "        y_train: Training targets.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting model training.\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2,callbacks=[early_stopping, reduce_lr])\n",
    "    logger.info(\"Model training complete.\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c9326df-2db0-4995-92bd-0d5a933b3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[1m1617/1617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438us/step\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generate_submission(model: Sequential, ais_test: pd.DataFrame, feature_scaler: MinMaxScaler, target_scaler: MinMaxScaler, vessel_ids: Dict) -> None:\n",
    "    \"\"\"Generate a submission file with the predicted vessel positions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        ais_test: DataFrame containing AIS test data.\n",
    "        feature_scaler: Scaler used to normalize the features.\n",
    "        target_scaler: Scaler used to normalize the target coordinates.\n",
    "        vessel_ids: Dictionary to map vessel IDs to numerical indices.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating predictions for the test set.\")\n",
    "    \n",
    "    # Convert the 'time' column to datetime format to handle arithmetic operations\n",
    "    ais_test['time'] = pd.to_datetime(ais_test['time'], errors='coerce')\n",
    "    \n",
    "    # Map vesselId to its encoded value using vessel_ids dictionary\n",
    "    ais_test['vesselId_encoded'] = ais_test['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "   \n",
    "    # Calculate the time elapsed since the first recorded entry for each vessel\n",
    "    ais_test['time_elapsed'] = (ais_test['time'] - ais_test['time'].min()).dt.total_seconds()\n",
    "\n",
    "    # Extract the relevant features for the test data\n",
    "    test_features = ais_test[['vesselId_encoded', 'time_elapsed']].values\n",
    "\n",
    "    # Since the test data only has one feature, we need to adjust the input shape to match the model's expectation\n",
    "    num_train_features = feature_scaler.n_features_in_\n",
    "    test_features_padded = np.zeros((test_features.shape[0], num_train_features))\n",
    "    test_features_padded[:, :test_features.shape[1]] = test_features\n",
    "\n",
    "    # Normalize the padded test features to match the training data scale\n",
    "    test_features_scaled = feature_scaler.transform(test_features_padded)\n",
    "    X_test = test_features_scaled.reshape((test_features_scaled.shape[0], 1, test_features_scaled.shape[1]))\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform the predictions using the target scaler\n",
    "    predictions = target_scaler.inverse_transform(predictions)\n",
    "    \n",
    "    # Create the submission DataFrame in the required format\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': ais_test['ID'],\n",
    "        'longitude_predicted': predictions[:, 1],\n",
    "        'latitude_predicted': predictions[:, 0]\n",
    "    })\n",
    "   \n",
    "    # Ensure that the submission file has exactly 51739 rows as required\n",
    "    assert submission.shape[0] == 51739, \"The submission file must have exactly 51739 rows.\"\n",
    "    \n",
    "    # Save the predictions to submission.csv\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    logger.info(\"Submission file saved as submission.csv.\")\n",
    "\n",
    "\n",
    "# Generate the submission file\n",
    "generate_submission(model, ais_test, feature_scaler, target_scaler, vessel_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccf0fcd-cdbb-44f4-8acc-5d62a2845a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Longitude Predicted: 28.37741\n",
      "Minimum Longitude Predicted: 3.8418357\n",
      "Maximum Latitude Predicted: 56.508556\n",
      "Minimum Latitude Predicted: 38.977943\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('submission.csv')\n",
    "\n",
    "# Calculate the maximum and minimum values for longitude_predicted and latitude_predicted\n",
    "longitude_max = df['longitude_predicted'].max()\n",
    "longitude_min = df['longitude_predicted'].min()\n",
    "latitude_max = df['latitude_predicted'].max()\n",
    "latitude_min = df['latitude_predicted'].min()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum Longitude Predicted: {longitude_max}\")\n",
    "print(f\"Minimum Longitude Predicted: {longitude_min}\")\n",
    "print(f\"Maximum Latitude Predicted: {latitude_max}\")\n",
    "print(f\"Minimum Latitude Predicted: {latitude_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e26519-4a82-4e70-bb5a-c8a6d30fc710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating nearest ports: 100%|██████████████████████████████████████████████████████████████████████████████| 1522065/1522065 [02:12<00:00, 11528.05vessel/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 553us/step - loss: 0.0093\n",
      "Epoch 2/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 552us/step - loss: 4.3165e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 552us/step - loss: 2.9144e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 555us/step - loss: 2.2083e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 550us/step - loss: 1.8692e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 558us/step - loss: 1.6073e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 560us/step - loss: 1.4307e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 561us/step - loss: 1.2737e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 582us/step - loss: 1.1609e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 558us/step - loss: 1.1753e-05\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 305us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 549us/step - loss: 2623.6301\n",
      "Epoch 2/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m719s\u001b[0m 15ms/step - loss: 2595.5369\n",
      "Epoch 3/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 552us/step - loss: 2587.8733\n",
      "Epoch 4/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 8ms/step - loss: 2576.3860\n",
      "Epoch 5/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 562us/step - loss: 2562.2231\n",
      "Epoch 6/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 551us/step - loss: 2534.6355\n",
      "Epoch 7/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 555us/step - loss: 2525.0088\n",
      "Epoch 8/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 569us/step - loss: 2505.4629\n",
      "Epoch 9/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 578us/step - loss: 2471.6233\n",
      "Epoch 10/10\n",
      "\u001b[1m47565/47565\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 558us/step - loss: 2445.7361\n",
      "\u001b[1m1617/1617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 329us/step\n",
      "Submission file created successfully with 51739 rows.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from rtree import index\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Load the data\n",
    "ais_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "ais_test = pd.read_csv('ais_test.csv')\n",
    "ports = pd.read_csv('ports.csv', sep='|')  # Assuming you have a ports.csv file to load port data\n",
    "\n",
    "# Measure time to create the R-tree index for ports\n",
    "start_time = time.time()\n",
    "port_idx = index.Index()\n",
    "for idx, row in ports.iterrows():\n",
    "    port_idx.insert(idx, (row['longitude'], row['latitude'], row['longitude'], row['latitude']))\n",
    "end_time = time.time()\n",
    "logger.info(f\"Time to build R-tree index for ports: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure time to calculate the nearest port for each vessel with progress and ETA\n",
    "total_vessels = len(ais_train)\n",
    "start_time = time.time()\n",
    "closest_ports = []\n",
    "\n",
    "for _, vessel in tqdm(ais_train.iterrows(), total=total_vessels, desc=\"Calculating nearest ports\", unit=\"vessel\"):\n",
    "    point = Point(vessel['longitude'], vessel['latitude'])\n",
    "    nearest_port_idx = list(port_idx.nearest((vessel['longitude'], vessel['latitude'], vessel['longitude'], vessel['latitude']), 1))[0]\n",
    "    closest_port = ports.iloc[nearest_port_idx]\n",
    "    distance_to_port = point.distance(Point(closest_port['longitude'], closest_port['latitude']))\n",
    "    closest_ports.append(distance_to_port)\n",
    "\n",
    "end_time = time.time()\n",
    "logger.info(f\"Time to calculate closest port for all vessels: {end_time - start_time:.2f} seconds for {total_vessels} vessels\")\n",
    "\n",
    "# Add the calculated distances to the AIS data\n",
    "ais_train['distance_to_nearest_port'] = closest_ports\n",
    "\n",
    "speed_threshold = 5.0  # knots\n",
    "distance_threshold = 1.0  # kilometers\n",
    "\n",
    "# Identify if the vessel is anchored\n",
    "ais_train['anchored'] = (ais_train['sog'] < speed_threshold) & (ais_train['distance_to_nearest_port'] < distance_threshold)\n",
    "\n",
    "# Convert 'time' column to datetime and extract useful features like hour, minute, and total time elapsed\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'], errors='coerce')\n",
    "ais_train['hour'] = ais_train['time'].dt.hour\n",
    "ais_train['minute'] = ais_train['time'].dt.minute\n",
    "\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time'], errors='coerce')\n",
    "ais_test['hour'] = ais_test['time'].dt.hour\n",
    "ais_test['minute'] = ais_test['time'].dt.minute\n",
    "\n",
    "# Calculate the total time elapsed in seconds from the earliest time in the dataset\n",
    "reference_time = ais_train['time'].min()\n",
    "ais_train['time_elapsed'] = (ais_train['time'] - reference_time).dt.total_seconds()\n",
    "ais_test['time_elapsed'] = (ais_test['time'] - reference_time).dt.total_seconds()\n",
    "\n",
    "# Fill missing values in 'hour', 'minute', and 'time_elapsed' with a placeholder (e.g., -1) to handle NaN values\n",
    "ais_train[['hour', 'minute', 'time_elapsed']] = ais_train[['hour', 'minute', 'time_elapsed']].fillna(-1)\n",
    "ais_test[['hour', 'minute', 'time_elapsed']] = ais_test[['hour', 'minute', 'time_elapsed']].fillna(-1)\n",
    "\n",
    "# Encode categorical features like 'vesselId' using Label Encoding\n",
    "le = LabelEncoder()\n",
    "ais_train['vesselId_encoded'] = le.fit_transform(ais_train['vesselId'].astype(str))\n",
    "ais_test['vesselId_encoded'] = le.transform(ais_test['vesselId'].astype(str))\n",
    "\n",
    "# Define numeric columns for training separately, including the new time_elapsed feature\n",
    "train_numeric_columns = ['cog', 'sog', 'rot', 'heading', 'navstat', 'etaRaw', 'latitude', 'longitude', 'vesselId_encoded', 'time_elapsed', 'distance_to_nearest_port', 'anchored']\n",
    "\n",
    "# Convert training columns to numeric, coercing errors to NaN\n",
    "ais_train[train_numeric_columns] = ais_train[train_numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill any remaining NaN values in the dataset with appropriate defaults\n",
    "ais_train[train_numeric_columns] = ais_train[train_numeric_columns].fillna(0)\n",
    "\n",
    "# Extract features and targets for the first model using the full training set\n",
    "X_train_full = ais_train[train_numeric_columns].values\n",
    "y_train_full = ais_train[['latitude', 'longitude']].values\n",
    "\n",
    "# Normalize/Scale the full training set\n",
    "scaler_X_train_full = StandardScaler()\n",
    "X_train_full = scaler_X_train_full.fit_transform(X_train_full)\n",
    "\n",
    "scaler_y_full = StandardScaler()\n",
    "y_train_full = scaler_y_full.fit_transform(y_train_full)\n",
    "\n",
    "# Reshape the data for LSTM input (samples, timesteps, features)\n",
    "X_train_full = X_train_full.reshape(X_train_full.shape[0], 1, X_train_full.shape[1])\n",
    "\n",
    "# Build and train the first model on the full training set\n",
    "input_full = Input(shape=(1, X_train_full.shape[2]), name='full_input')\n",
    "lstm_full = LSTM(64, return_sequences=False)(input_full)\n",
    "dense_full = Dense(32, activation='relu')(lstm_full)\n",
    "output_full = Dense(2, activation='linear', name='coordinates_output')(dense_full)\n",
    "\n",
    "model_full = Model(inputs=input_full, outputs=output_full)\n",
    "model_full.compile(optimizer='adam', loss='mse')\n",
    "model_full.fit(X_train_full, y_train_full, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the full training set using the first model\n",
    "predicted_coords = model_full.predict(X_train_full)\n",
    "predicted_coords = scaler_y_full.inverse_transform(predicted_coords)  # Reverse scaling\n",
    "\n",
    "# Prepare input data for the second model using vesselId, hour, minute, and time_elapsed features\n",
    "X_train_second_model = ais_train[['vesselId_encoded', 'time_elapsed']].values\n",
    "y_train_second_model = predicted_coords  # Use the predicted coordinates as labels\n",
    "\n",
    "# Normalize/Scale the input data for the second model\n",
    "scaler_X_second = StandardScaler()\n",
    "X_train_second_model = scaler_X_second.fit_transform(X_train_second_model)\n",
    "\n",
    "# Reshape the data for LSTM input (samples, timesteps, features)\n",
    "X_train_second_model = X_train_second_model.reshape(X_train_second_model.shape[0], 1, X_train_second_model.shape[1])\n",
    "\n",
    "# Build and train the second model on vesselId, time, and time_elapsed\n",
    "input_second = Input(shape=(1, X_train_second_model.shape[2]), name='second_input')\n",
    "lstm_second = LSTM(64, return_sequences=False)(input_second)\n",
    "dense_second = Dense(32, activation='relu')(lstm_second)\n",
    "output_second = Dense(2, activation='linear', name='coordinates_predicted')(dense_second)\n",
    "\n",
    "model_second = Model(inputs=input_second, outputs=output_second)\n",
    "model_second.compile(optimizer='adam', loss='mse')\n",
    "model_second.fit(X_train_second_model, y_train_second_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the test set using the second model\n",
    "X_test_subset = ais_test[['vesselId_encoded', 'time_elapsed']].values\n",
    "X_test_subset = scaler_X_second.transform(X_test_subset)\n",
    "X_test_subset = X_test_subset.reshape(X_test_subset.shape[0], 1, X_test_subset.shape[1])\n",
    "\n",
    "test_predictions = model_second.predict(X_test_subset)\n",
    "\n",
    "# Reverse the scaling to get the original longitude and latitude values\n",
    "test_predictions = scaler_y_full.inverse_transform(test_predictions)\n",
    "\n",
    "# Generate the submission file with the predicted vessel positions\n",
    "submission = pd.DataFrame({\n",
    "    'ID': ais_test['ID'],\n",
    "    'longitude_predicted': test_predictions[:, 1],\n",
    "    'latitude_predicted': test_predictions[:, 0]\n",
    "})\n",
    "\n",
    "# Ensure that the submission file has exactly 51739 rows as required\n",
    "assert submission.shape[0] == 51739, \"The submission file must have exactly 51739 rows.\"\n",
    "\n",
    "# Save the submission to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully with 51739 rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
