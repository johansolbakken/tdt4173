{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0000e4a5-4ad3-4dc8-9a51-02d465822eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "from typing import Tuple, Dict\n",
    "from colorlog import ColoredFormatter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8156b3c2-2141-4d48-bb8b-6e4db74e9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the colorful logger\n",
    "def setup_logger() -> logging.Logger:\n",
    "    \"\"\"Set up a colorful logger for the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"ML_Pipeline\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Define log colors for different levels\n",
    "    formatter = ColoredFormatter(\n",
    "        \"%(log_color)s%(levelname)-8s%(reset)s | %(log_color)s%(message)s%(reset)s\",\n",
    "        datefmt=None,\n",
    "        log_colors={\n",
    "            'DEBUG':    'cyan',\n",
    "            'INFO':     'white',\n",
    "            'WARNING':  'yellow',\n",
    "            'ERROR':    'red',\n",
    "            'CRITICAL': 'bold_red',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Stream handler for console output\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize the logger\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895abb9e-1bb5-4546-baa4-094df5cd2616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mLoading datasets.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mLoading datasets.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mLoading datasets.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mDatasets loaded successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mDatasets loaded successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mDatasets loaded successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load the AIS and auxiliary datasets.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the loaded DataFrames:\n",
    "        (ais_train, ais_test, vessels, ports, schedules).\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading datasets.\")\n",
    "    ais_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "    ais_test = pd.read_csv('ais_test.csv')\n",
    "    vessels = pd.read_csv('vessels.csv', sep='|')\n",
    "    ports = pd.read_csv('ports.csv', sep='|')\n",
    "    schedules = pd.read_csv('schedules_to_may_2024.csv', sep='|', on_bad_lines='skip')\n",
    "    logger.info(\"Datasets loaded successfully.\")\n",
    "    return ais_train, ais_test, vessels, ports, schedules\n",
    "\n",
    "# Load the data\n",
    "ais_train, ais_test, vessels, ports, schedules = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22634fa2-f653-4069-b56f-5f128fb6053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mPreparing data for the model.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating Missing Days For Vessels: 100%|██████████████████████████| 711/711 [02:05<00:00,  5.67vessel/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mData preparation complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mData preparation complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mData preparation complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def prepare_data(\n",
    "    ais_train: pd.DataFrame,\n",
    "    vessels: pd.DataFrame, \n",
    "    ports: pd.DataFrame, \n",
    "    schedules: pd.DataFrame,\n",
    "    vessel_ids: Dict\n",
    ") -> Tuple[np.ndarray, np.ndarray, MinMaxScaler, MinMaxScaler]:\n",
    "    \"\"\"Prepare the data for the LSTM model, including additional time-based, vessel-specific, and port proximity features.\n",
    "    \n",
    "    Args:\n",
    "        ais_train: DataFrame containing AIS training data.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing the feature array (X), target array (y), and the fitted scaler.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing data for the model.\")\n",
    "    \n",
    "    # Convert the 'time' column to datetime format for feature extraction\n",
    "    ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "    for col in ['latitude', 'longitude', 'cog', 'sog', 'rot', 'heading', 'etaRaw']:\n",
    "        ais_train[col] = pd.to_numeric(ais_train[col], errors='coerce')\n",
    "\n",
    "    # Clip latitude and longitude to their valid ranges\n",
    "    ais_train['latitude'] = ais_train['latitude'].clip(-90, 90)\n",
    "    ais_train['longitude'] = ais_train['longitude'].clip(-180, 180)\n",
    "\n",
    "    # List to store the interpolated data points\n",
    "    interpolated_data = []\n",
    "\n",
    "    # Loop through each vessel ID\n",
    "    for vessel_id in tqdm(vessel_ids.keys(), desc=\"Interpolating Missing Days For Vessels\", unit=\"vessel\"):\n",
    "        # Filter and sort the data for the current vessel ID\n",
    "        vessel_data = ais_train[ais_train['vesselId'] == vessel_id].sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "        # Proceed only if vessel_data is not empty\n",
    "        if len(vessel_data) > 0:\n",
    "            # Loop through the sorted data to check time differences\n",
    "            for i in range(len(vessel_data) - 1):\n",
    "                current_row = vessel_data.iloc[i]\n",
    "                next_row = vessel_data.iloc[i + 1]\n",
    "\n",
    "                time_difference = next_row['time'] - current_row['time']\n",
    "                \n",
    "                # Add the current row to the interpolated data list\n",
    "                interpolated_data.append(current_row)\n",
    "\n",
    "                # Check if the time difference is greater than 1 day\n",
    "                if time_difference > pd.Timedelta(days=1):\n",
    "                    # Calculate the number of missing days\n",
    "                    num_missing_days = (time_difference.days - 1)\n",
    "\n",
    "                    # Linearly interpolate values for each missing day\n",
    "                    for day in range(1, num_missing_days + 1):\n",
    "                        interpolated_time = current_row['time'] + pd.Timedelta(days=day)\n",
    "                        \n",
    "                        # Interpolate all relevant columns\n",
    "                        interpolated_values = {}\n",
    "                        for col in ['latitude', 'longitude']:\n",
    "                            value_diff = (next_row[col] - current_row[col]) / (num_missing_days + 1)\n",
    "                            interpolated_values[col] = current_row[col] + value_diff * day\n",
    "                        \n",
    "                        # Create a new interpolated data point\n",
    "                        interpolated_point = current_row.copy()\n",
    "                        interpolated_point['time'] = interpolated_time\n",
    "                        interpolated_point['latitude'] = interpolated_values['latitude']\n",
    "                        interpolated_point['longitude'] = interpolated_values['longitude']\n",
    "                        interpolated_point['vesselId'] = current_row['vesselId']\n",
    "\n",
    "                        # Add the interpolated point to the list\n",
    "                        interpolated_data.append(interpolated_point)\n",
    "\n",
    "            # Add the last row to the interpolated data list\n",
    "            interpolated_data.append(vessel_data.iloc[-1])\n",
    "    \n",
    "    # Convert the list of interpolated data back into a DataFrame\n",
    "    interpolated_df = pd.DataFrame(interpolated_data)\n",
    "\n",
    "    # Combine the interpolated data with the original ais_train DataFrame\n",
    "    combined_df = pd.concat([ais_train, interpolated_df]).drop_duplicates().sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    ais_train = combined_df\n",
    "\n",
    "    # Calculate the time elapsed since the first recorded entry for each vessel\n",
    "    ais_train['time_elapsed'] = (ais_train['time'] - ais_train['time'].min()).dt.total_seconds()\n",
    "   \n",
    "    # Map vesselId to its encoded value using vessel_ids dictionary\n",
    "    ais_train['vesselId_encoded'] = ais_train['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "\n",
    "    # Extract the relevant features, including the new ones\n",
    "    features = ais_train[['vesselId_encoded', 'time_elapsed']].values\n",
    "    target = ais_train[['latitude', 'longitude']].shift(-1).ffill().values\n",
    "\n",
    "    # Normalize features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "    \n",
    "    y = ais_train[['latitude', 'longitude']].shift(-1).ffill().values\n",
    "\n",
    "    # Reshape for LSTM input: (samples, timesteps, features)\n",
    "    X = features_scaled.reshape((features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
    "    \n",
    "    # Normalize target data (latitude and longitude)\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y = target_scaler.fit_transform(target)    \n",
    "    \n",
    "    logger.info(\"Data preparation complete.\")\n",
    "    return X, y, feature_scaler, target_scaler\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "vessel_id_dict = {row[\"vesselId\"]: i for i, row in vessels.iterrows()}\n",
    "X_train, y_train, feature_scaler, target_scaler = prepare_data(ais_train, vessels, ports, schedules, vessel_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed2a8d6-c490-498a-bb43-6af56b5fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def geodesic_loss(y_true, y_pred):\n",
    "    \"\"\"Calculate the Haversine distance between true and predicted coordinates.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor of true coordinates (latitude, longitude).\n",
    "        y_pred: Tensor of predicted coordinates (latitude, longitude).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor representing the geodesic distance (Haversine distance) between the true and predicted points.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Split the latitude and longitude into separate tensors\n",
    "    lat_true, lon_true = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
    "    lat_pred, lon_pred = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "    \n",
    "    # Convert degrees to radians manually\n",
    "    lat_true = lat_true * tf.constant(np.pi / 180.0)\n",
    "    lon_true = lon_true * tf.constant(np.pi / 180.0)\n",
    "    lat_pred = lat_pred * tf.constant(np.pi / 180.0)\n",
    "    lon_pred = lon_pred * tf.constant(np.pi / 180.0)\n",
    "    \n",
    "    # Compute the differences between true and predicted coordinates\n",
    "    dlat = lat_pred - lat_true\n",
    "    dlon = lon_pred - lon_true\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = tf.square(tf.sin(dlat / 2)) + tf.cos(lat_true) * tf.cos(lat_pred) * tf.square(tf.sin(dlon / 2))\n",
    "    c = 2 * tf.atan2(tf.sqrt(a), tf.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return tf.reduce_mean(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f38c7f-8f1d-489a-b718-7b0547cd1c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mBuilding the LSTM model.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel built successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape: Tuple[int, int]) -> Sequential:\n",
    "    \"\"\"Build the LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of the input data (timesteps, features).\n",
    "        \n",
    "    Returns:\n",
    "        Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Building the LSTM model.\")\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))  # Use Input layer to specify the shape\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=2))  # Output: latitude and longitude\n",
    "    model.compile(optimizer='adam', loss=geodesic_loss)\n",
    "    logger.info(\"Model built successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=(X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "892aa819-c46d-465a-8df1-0cb4101faf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mStarting model training.\u001b[0m\n",
      "Epoch 1/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 934us/step - loss: 24.9886 - val_loss: 35.4740 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 975us/step - loss: 23.6152 - val_loss: 19.8003 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1ms/step - loss: 23.5554 - val_loss: 25.7895 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1ms/step - loss: 23.5387 - val_loss: 27.1325 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1ms/step - loss: 23.4886 - val_loss: 27.8399 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1ms/step - loss: 23.4468 - val_loss: 26.8645 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m38965/38965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1ms/step - loss: 23.4379 - val_loss: 27.5344 - learning_rate: 5.0000e-04\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mModel training complete.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def train_model(model: Sequential, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "    \"\"\"Train the LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: The LSTM model to train.\n",
    "        X_train: Training features.\n",
    "        y_train: Training targets.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting model training.\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2,callbacks=[early_stopping, reduce_lr])\n",
    "    logger.info(\"Model training complete.\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c9326df-2db0-4995-92bd-0d5a933b3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mGenerating predictions for the test set.\u001b[0m\n",
      "\u001b[1m1617/1617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438us/step\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n",
      "\u001b[37mINFO    \u001b[0m | \u001b[37mSubmission file saved as submission.csv.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generate_submission(model: Sequential, ais_test: pd.DataFrame, feature_scaler: MinMaxScaler, target_scaler: MinMaxScaler, vessel_ids: Dict) -> None:\n",
    "    \"\"\"Generate a submission file with the predicted vessel positions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        ais_test: DataFrame containing AIS test data.\n",
    "        feature_scaler: Scaler used to normalize the features.\n",
    "        target_scaler: Scaler used to normalize the target coordinates.\n",
    "        vessel_ids: Dictionary to map vessel IDs to numerical indices.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating predictions for the test set.\")\n",
    "    \n",
    "    # Convert the 'time' column to datetime format to handle arithmetic operations\n",
    "    ais_test['time'] = pd.to_datetime(ais_test['time'], errors='coerce')\n",
    "    \n",
    "    # Map vesselId to its encoded value using vessel_ids dictionary\n",
    "    ais_test['vesselId_encoded'] = ais_test['vesselId'].map(vessel_ids).fillna(-1).astype(int)\n",
    "   \n",
    "    # Calculate the time elapsed since the first recorded entry for each vessel\n",
    "    ais_test['time_elapsed'] = (ais_test['time'] - ais_test['time'].min()).dt.total_seconds()\n",
    "\n",
    "    # Extract the relevant features for the test data\n",
    "    test_features = ais_test[['vesselId_encoded', 'time_elapsed']].values\n",
    "\n",
    "    # Since the test data only has one feature, we need to adjust the input shape to match the model's expectation\n",
    "    num_train_features = feature_scaler.n_features_in_\n",
    "    test_features_padded = np.zeros((test_features.shape[0], num_train_features))\n",
    "    test_features_padded[:, :test_features.shape[1]] = test_features\n",
    "\n",
    "    # Normalize the padded test features to match the training data scale\n",
    "    test_features_scaled = feature_scaler.transform(test_features_padded)\n",
    "    X_test = test_features_scaled.reshape((test_features_scaled.shape[0], 1, test_features_scaled.shape[1]))\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform the predictions using the target scaler\n",
    "    predictions = target_scaler.inverse_transform(predictions)\n",
    "    \n",
    "    # Create the submission DataFrame in the required format\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': ais_test['ID'],\n",
    "        'longitude_predicted': predictions[:, 1],\n",
    "        'latitude_predicted': predictions[:, 0]\n",
    "    })\n",
    "   \n",
    "    # Ensure that the submission file has exactly 51739 rows as required\n",
    "    assert submission.shape[0] == 51739, \"The submission file must have exactly 51739 rows.\"\n",
    "    \n",
    "    # Save the predictions to submission.csv\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    logger.info(\"Submission file saved as submission.csv.\")\n",
    "\n",
    "\n",
    "# Generate the submission file\n",
    "generate_submission(model, ais_test, feature_scaler, target_scaler, vessel_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccf0fcd-cdbb-44f4-8acc-5d62a2845a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Longitude Predicted: 28.37741\n",
      "Minimum Longitude Predicted: 3.8418357\n",
      "Maximum Latitude Predicted: 56.508556\n",
      "Minimum Latitude Predicted: 38.977943\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('submission.csv')\n",
    "\n",
    "# Calculate the maximum and minimum values for longitude_predicted and latitude_predicted\n",
    "longitude_max = df['longitude_predicted'].max()\n",
    "longitude_min = df['longitude_predicted'].min()\n",
    "latitude_max = df['latitude_predicted'].max()\n",
    "latitude_min = df['latitude_predicted'].min()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum Longitude Predicted: {longitude_max}\")\n",
    "print(f\"Minimum Longitude Predicted: {longitude_min}\")\n",
    "print(f\"Maximum Latitude Predicted: {latitude_max}\")\n",
    "print(f\"Minimum Latitude Predicted: {latitude_min}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
