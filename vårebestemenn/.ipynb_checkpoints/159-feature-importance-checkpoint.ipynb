{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd315bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from darts import TimeSeries\n",
    "from darts.models import LightGBMModel\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13a542a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ais_train.csv with separator '|'\n",
    "train_df = pd.read_csv(\"../project/ais_train.csv\", sep=\"|\")\n",
    "train_df[\"time\"] = pd.to_datetime(train_df[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "963e9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ais_test.csv with separator ','\n",
    "test_df = pd.read_csv(\"../project/ais_test.csv\", sep=\",\")\n",
    "test_df[\"time\"] = pd.to_datetime(test_df[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f237d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'vesselId' instead of 'vessel_id'\n",
    "# Select only vessel IDs that are in both train and test datasets\n",
    "common_vessel_ids = set(train_df[\"vesselId\"]).intersection(set(test_df[\"vesselId\"]))\n",
    "train_df = train_df[train_df[\"vesselId\"].isin(common_vessel_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13e2d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the training data by vesselId\n",
    "groups = train_df.groupby(\"vesselId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e841ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store TimeSeries objects, last training times, and feature importances\n",
    "timeseries_dict = {}\n",
    "last_train_time = {}\n",
    "feature_importances = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "892d0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each vesselId group\n",
    "for vessel_id, group_df in groups:\n",
    "    # Sort on time\n",
    "    group_df = group_df.sort_values(\"time\")\n",
    "    # Set index to time\n",
    "    group_df = group_df.set_index(\"time\")\n",
    "    # Select features (latitude and longitude)\n",
    "    features_df = group_df[[\"latitude\", \"longitude\"]]\n",
    "    # Resample data to hourly frequency with mean and linear interpolation\n",
    "    features_df = features_df.resample(\"h\").mean().interpolate(method=\"cubic\")\n",
    "    # Create Darts TimeSeries object\n",
    "    ts = TimeSeries.from_dataframe(features_df, value_cols=[\"latitude\", \"longitude\"])\n",
    "    # Store the TimeSeries object and last training time\n",
    "    timeseries_dict[vessel_id] = ts\n",
    "    last_train_time[vessel_id] = features_df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72feeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store predictions\n",
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b873363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24480\n",
      "[LightGBM] [Info] Number of data points in the train set: 3024, number of used features: 96\n",
      "[LightGBM] [Info] Start training from score 44.152453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24480\n",
      "[LightGBM] [Info] Number of data points in the train set: 3024, number of used features: 96\n",
      "[LightGBM] [Info] Start training from score -4.685515\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiOutputRegressor' object has no attribute 'feature_importance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Extract feature importance from the LightGBM model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Access the underlying LightGBM model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m importance \u001b[38;5;241m=\u001b[39m \u001b[43mlgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importance\u001b[49m(importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m feature_importances[vessel_id] \u001b[38;5;241m=\u001b[39m importance\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiOutputRegressor' object has no attribute 'feature_importance'"
     ]
    }
   ],
   "source": [
    "# Fit LightGBM models, predict, and compute feature importance for each TimeSeries object\n",
    "for vessel_id, ts in timeseries_dict.items():\n",
    "    # Get the last training time\n",
    "    last_time = last_train_time[vessel_id]\n",
    "    # Get test times for this vessel\n",
    "    vessel_test_df = test_df[test_df[\"vesselId\"] == vessel_id]\n",
    "    test_times = vessel_test_df[\"time\"]\n",
    "    # Compute the time differences in hours\n",
    "    time_diffs = (test_times - last_time).dt.total_seconds() / 3600\n",
    "    # Get the maximum forecast horizon needed\n",
    "    max_n = int(np.ceil(time_diffs.max()))\n",
    "    if max_n <= 0:\n",
    "        continue  # Skip if no future times to predict\n",
    "    # Initialize LightGBM model with lag parameters\n",
    "    model = LightGBMModel(\n",
    "        lags=48,  # the correct value is between 48 and 96\n",
    "        learning_rate=0.1,\n",
    "    )\n",
    "    # Fit the model\n",
    "    model.fit(ts)\n",
    "    # Predict up to the maximum horizon needed\n",
    "    forecast = model.predict(max_n)\n",
    "    # Store the forecast and last time\n",
    "    predictions[vessel_id] = (forecast, last_time)\n",
    "\n",
    "    # Extract feature importance from the LightGBM model\n",
    "    lgb_model = model.model  # Access the underlying LightGBM model\n",
    "    importance = lgb_model.feature_importance(importance_type=\"split\")\n",
    "    feature_importances[vessel_id] = importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importances\n",
    "for vessel_id, importance in feature_importances.items():\n",
    "    print(f\"Feature importance for vessel {vessel_id}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store submission rows\n",
    "submission_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the submission file\n",
    "for idx, row in test_df.iterrows():\n",
    "    vessel_id = row[\"vesselId\"]\n",
    "    test_time = row[\"time\"]\n",
    "    test_id = row[\"ID\"]  # Assuming 'ID' column exists in test_df\n",
    "    # Check if predictions are available for this vessel_id\n",
    "    if vessel_id in predictions:\n",
    "        forecast_ts, last_time = predictions[vessel_id]\n",
    "        time_diff = (test_time - last_time).total_seconds() / 3600\n",
    "        index = (\n",
    "            int(np.round(time_diff)) - 1\n",
    "        )  # Adjust index since forecast starts from last_time + 1 hour\n",
    "        # Convert forecast_ts to DataFrame\n",
    "        forecast_df = forecast_ts.pd_dataframe()\n",
    "        # Check if index is within forecast horizon\n",
    "        if 0 <= index < len(forecast_df):\n",
    "            predicted_lat = forecast_df[\"latitude\"].iloc[index]\n",
    "            predicted_lon = forecast_df[\"longitude\"].iloc[index]\n",
    "        else:\n",
    "            predicted_lat = np.nan\n",
    "            predicted_lon = np.nan\n",
    "    else:\n",
    "        predicted_lat = np.nan\n",
    "        predicted_lon = np.nan\n",
    "    # Append the prediction to the submission list\n",
    "    submission_rows.append(\n",
    "        {\n",
    "            \"ID\": test_id,\n",
    "            \"longitude_predicted\": predicted_lon,\n",
    "            \"latitude_predicted\": predicted_lat,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55820085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission DataFrame from the list\n",
    "submission_df = pd.DataFrame(submission_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9689a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e29868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  longitude_predicted  latitude_predicted\n",
      "0          0                  NaN                 NaN\n",
      "1          1                  NaN                 NaN\n",
      "2          2                  NaN                 NaN\n",
      "3          3                  NaN                 NaN\n",
      "4          4                  NaN                 NaN\n",
      "...      ...                  ...                 ...\n",
      "51734  51734                  NaN                 NaN\n",
      "51735  51735                  NaN                 NaN\n",
      "51736  51736                  NaN                 NaN\n",
      "51737  51737                  NaN                 NaN\n",
      "51738  51738                  NaN                 NaN\n",
      "\n",
      "[51739 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(submission_df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "darts-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
